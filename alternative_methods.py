# -*- coding: utf-8 -*-
"""
ä»£æ›¿ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°æ‰‹æ³•ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«: Agentic Clustering v0.8
GMM, DBSCANãªã©ã®ä»£æ›¿ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã‚’æä¾›
HDBSCAN, CLASSIXã®ãƒ™ã‚¤ã‚ºæœ€é©åŒ–ã®æ¢ç´¢ã‚‚å«ã‚€
"""

import numpy as np
from sklearn.mixture import GaussianMixture
from sklearn.cluster import DBSCAN, KMeans
from sklearn.metrics import silhouette_score
import config

class AlternativeClusteringMethods:
    """ä»£æ›¿ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°æ‰‹æ³•ã‚’æä¾›ã™ã‚‹ã‚¯ãƒ©ã‚¹"""
    
    def __init__(self, X_scaled):
        """
        Parameters:
        -----------
        X_scaled : array-like
            æ¨™æº–åŒ–ã•ã‚ŒãŸç‰¹å¾´é‡
        """
        self.X_scaled = X_scaled
        self.results = {}
    
    def try_kmeans(self, n_clusters):
        """KMeansã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°"""
        print(f"\nğŸ”µ KMeans (k={n_clusters}) ã‚’å®Ÿè¡Œä¸­...")
        
        kmeans = KMeans(
            n_clusters=n_clusters,
            random_state=config.RANDOM_STATE,
            n_init=10,
            max_iter=300
        )
        
        labels = kmeans.fit_predict(self.X_scaled)
        
        # ã‚¯ãƒ©ã‚¹ã‚¿åˆ†å¸ƒã‚’è¡¨ç¤º
        unique, counts = np.unique(labels, return_counts=True)
        print(f"   ã‚¯ãƒ©ã‚¹ã‚¿æ•°: {len(unique)}")
        for cluster_id, count in zip(unique, counts):
            print(f"   ã‚¯ãƒ©ã‚¹ã‚¿ {cluster_id}: {count}ä»¶")
        
        self.results['KMeans'] = {
            'model': kmeans,
            'labels': labels,
            'n_clusters': n_clusters
        }
        
        return labels
    
    def try_gmm(self, n_components_range=None, use_bayesian=False, n_calls=100):
        """ã‚¬ã‚¦ã‚¹æ··åˆãƒ¢ãƒ‡ãƒ«ï¼ˆGMMï¼‰
        
        Parameters:
        -----------
        n_components_range : range, optional
            æ¢ç´¢ã™ã‚‹ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆæ•°ã®ç¯„å›²ï¼ˆã‚°ãƒªãƒƒãƒ‰ã‚µãƒ¼ãƒã®å ´åˆï¼‰
        use_bayesian : bool, default=False
            Trueã®å ´åˆã€ãƒ™ã‚¤ã‚ºæœ€é©åŒ–ã‚’ä½¿ç”¨
        n_calls : int, default=100
            ãƒ™ã‚¤ã‚ºæœ€é©åŒ–ã®è©•ä¾¡å›æ•°
        """
        print(f"\nğŸŸ£ GMM (Gaussian Mixture Model) ã‚’å®Ÿè¡Œä¸­...")
        
        # ãƒ™ã‚¤ã‚ºæœ€é©åŒ–ã‚’è©¦è¡Œ
        if use_bayesian:
            try:
                from skopt import gp_minimize
                from skopt.space import Integer
                from skopt.utils import use_named_args
                print(f"   ğŸ”¬ ãƒ™ã‚¤ã‚ºæœ€é©åŒ–ã§ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ¢ç´¢ã‚’å®Ÿè¡Œã—ã¾ã™ï¼ˆ{n_calls}å›è©•ä¾¡ï¼‰")
                return self._try_gmm_bayesian(n_calls=n_calls)
            except ImportError:
                print("   âš ï¸ scikit-optimizeãŒåˆ©ç”¨ã§ãã¾ã›ã‚“ã€‚ã‚°ãƒªãƒƒãƒ‰ã‚µãƒ¼ãƒã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ã—ã¾ã™ã€‚")
        
        # ã‚°ãƒªãƒƒãƒ‰ã‚µãƒ¼ãƒ
        if n_components_range is None:
            n_components_range = range(config.MIN_CLUSTERS, config.MAX_CLUSTERS + 1)
        
        best_gmm = None
        best_labels = None
        best_score = -1
        best_n = config.MIN_CLUSTERS
        
        for n in n_components_range:
            gmm = GaussianMixture(
                n_components=n,
                covariance_type='full',
                random_state=config.RANDOM_STATE,
                n_init=10
            )
            
            labels = gmm.fit_predict(self.X_scaled)
            
            # ãƒã‚¤ã‚ºã‚¯ãƒ©ã‚¹ã‚¿ï¼ˆ-1ï¼‰ãŒã‚ã‚‹å ´åˆã¯é™¤å¤–ã—ã¦è©•ä¾¡
            if len(np.unique(labels)) > 1:
                score = silhouette_score(self.X_scaled, labels)
                print(f"   n_components={n}: ã‚·ãƒ«ã‚¨ãƒƒãƒˆã‚¹ã‚³ã‚¢ = {score:.4f}")
                
                if score > best_score:
                    best_score = score
                    best_gmm = gmm
                    best_labels = labels
                    best_n = n
        
        print(f"   âœ“ æœ€é©ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆæ•°: {best_n} (ã‚¹ã‚³ã‚¢: {best_score:.4f})")
        
        # ã‚¯ãƒ©ã‚¹ã‚¿åˆ†å¸ƒã‚’è¡¨ç¤º
        unique, counts = np.unique(best_labels, return_counts=True)
        print(f"   ã‚¯ãƒ©ã‚¹ã‚¿æ•°: {len(unique)}")
        for cluster_id, count in zip(unique, counts):
            print(f"   ã‚¯ãƒ©ã‚¹ã‚¿ {cluster_id}: {count}ä»¶")
        
        self.results['GMM'] = {
            'model': best_gmm,
            'labels': best_labels,
            'n_clusters': best_n,
            'score': best_score
        }
        
        return best_labels
    
    def _try_gmm_bayesian(self, n_calls=100):
        """GMMã®ãƒ™ã‚¤ã‚ºæœ€é©åŒ–
        
        Parameters:
        -----------
        n_calls : int
            ãƒ™ã‚¤ã‚ºæœ€é©åŒ–ã®è©•ä¾¡å›æ•°
        """
        from skopt import forest_minimize  # ä¸¦åˆ—åŒ–ã«é©ã—ãŸæ‰‹æ³•
        from skopt.space import Integer
        from skopt.utils import use_named_args
        from sklearn.metrics import davies_bouldin_score
        
        # æ¢ç´¢ç©ºé–“ã®å®šç¾©ï¼ˆn_componentsã¨covariance_typeï¼‰
        # covariance_typeã¯æ•´æ•°ã§ãƒãƒƒãƒ”ãƒ³ã‚°: 0=full, 1=tied, 2=diag, 3=spherical
        space = [
            Integer(10, 76, name='n_components'),  # ã‚¯ãƒ©ã‚¹ã‚¿æ•°: 10-76
            Integer(0, 3, name='cov_type_idx')     # å…±åˆ†æ•£ã‚¿ã‚¤ãƒ—: 0-3
        ]
        
        cov_types = ['full', 'tied', 'diag', 'spherical']
        
        @use_named_args(space)
        def objective(n_components, cov_type_idx):
            """æœ€å°åŒ–ã™ã‚‹ç›®çš„é–¢æ•°ï¼ˆè² ã®ã‚¹ã‚³ã‚¢ã‚’è¿”ã™ï¼‰"""
            try:
                cov_type = cov_types[cov_type_idx]
                
                # GMMãƒ¢ãƒ‡ãƒ«ã®æ§‹ç¯‰ã¨è¨“ç·´
                gmm = GaussianMixture(
                    n_components=n_components,
                    covariance_type=cov_type,
                    random_state=config.RANDOM_STATE,
                    n_init=5,  # ãƒ™ã‚¤ã‚ºæœ€é©åŒ–ã§ã¯å„è©•ä¾¡ã‚’é«˜é€ŸåŒ–
                    max_iter=100
                )
                
                labels = gmm.fit_predict(self.X_scaled)
                n_clusters = len(np.unique(labels))
                
                # ã‚¯ãƒ©ã‚¹ã‚¿ãŒ1ã¤ã ã‘ã®å ´åˆã¯ãƒšãƒŠãƒ«ãƒ†ã‚£
                if n_clusters < 2:
                    return 0.0  # æœ€æ‚ªã®ã‚¹ã‚³ã‚¢
                
                # ã‚·ãƒ«ã‚¨ãƒƒãƒˆã‚¹ã‚³ã‚¢ï¼ˆ0-1, é«˜ã„ã»ã©è‰¯ã„ï¼‰
                silhouette = silhouette_score(self.X_scaled, labels)
                
                # Davis-Bouldin Indexï¼ˆ0ä»¥ä¸Šã€ä½ã„ã»ã©è‰¯ã„ â†’ æ­£è¦åŒ–ï¼‰
                db_index = davies_bouldin_score(self.X_scaled, labels)
                db_normalized = max(0, 1.0 - db_index / 3.0)  # 3.0ã§å‰²ã£ã¦0-1ã«æ­£è¦åŒ–
                
                # ã‚¯ãƒ©ã‚¹ã‚¿ãƒãƒ©ãƒ³ã‚¹ã‚¹ã‚³ã‚¢ï¼ˆæ¨™æº–åå·®ãƒ™ãƒ¼ã‚¹ï¼‰
                unique_labels, counts = np.unique(labels, return_counts=True)
                if len(counts) > 1:
                    std_count = counts.std()
                    mean_count = counts.mean()
                    balance_score = max(0, 1.0 - std_count / (mean_count + 1e-6))
                else:
                    balance_score = 0.0
                
                # è¤‡åˆã‚¹ã‚³ã‚¢ (0.35:0.35:0.3 = Silhouette:DB:Balance)
                combined_score = 0.35 * silhouette + 0.35 * db_normalized + 0.3 * balance_score
                
                # ã‚¯ãƒ©ã‚¹ã‚¿æ•°ãƒšãƒŠãƒ«ãƒ†ã‚£ï¼ˆ10ä»¥ä¸Šã€76ä»¥ä¸‹ã‚’å¼·åˆ¶ï¼‰
                if n_clusters < 10:
                    combined_score *= 0.1  # 90%ãƒšãƒŠãƒ«ãƒ†ã‚£
                elif n_clusters < 15:
                    combined_score *= 0.6  # 40%ãƒšãƒŠãƒ«ãƒ†ã‚£
                elif n_clusters > 76:
                    combined_score *= 0.1  # 90%ãƒšãƒŠãƒ«ãƒ†ã‚£
                elif n_clusters > 60:
                    combined_score *= 0.7  # 30%ãƒšãƒŠãƒ«ãƒ†ã‚£
                
                return -combined_score  # æœ€å°åŒ–ã™ã‚‹ãŸã‚è² ã®å€¤ã‚’è¿”ã™
                
            except Exception as e:
                print(f"   âš ï¸ GMMè©•ä¾¡ã‚¨ãƒ©ãƒ¼ (n={n_components}, cov={cov_types[cov_type_idx]}): {e}")
                return 0.0  # ã‚¨ãƒ©ãƒ¼æ™‚ã¯æœ€æ‚ªã®ã‚¹ã‚³ã‚¢
        
        # é€²æ—è¡¨ç¤ºç”¨ã®ã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯
        iteration = [0]
        def on_step(res):
            iteration[0] += 1
            if iteration[0] % 10 == 0 or iteration[0] <= 5:
                print(f"   è©•ä¾¡ {iteration[0]}/{n_calls}: ç¾åœ¨ã®æœ€è‰¯ã‚¹ã‚³ã‚¢ = {-res.fun:.4f}")
        
        # ãƒ™ã‚¤ã‚ºæœ€é©åŒ–å®Ÿè¡Œï¼ˆforest_minimizeã§ä¸¦åˆ—å‡¦ç†æœ‰åŠ¹ï¼‰
        cores_msg = "å…¨CPUã‚³ã‚¢" if config.N_JOBS == -1 else f"{config.N_JOBS}ã‚³ã‚¢"
        print(f"   ğŸš€ Random Forestãƒ™ãƒ¼ã‚¹ã§{cores_msg}ä¸¦åˆ—å‡¦ç†ã‚’å®Ÿè¡Œ...")
        print(f"   ğŸ“Š åˆæœŸãƒ©ãƒ³ãƒ€ãƒ æ¢ç´¢: {config.N_INITIAL_POINTS}å›(ä¸¦åˆ—)ã€é€æ¬¡æ¢ç´¢: {n_calls - config.N_INITIAL_POINTS}å›")
        result = forest_minimize(
            objective,
            space,
            n_calls=n_calls,
            n_initial_points=config.N_INITIAL_POINTS,
            n_jobs=config.N_JOBS,  # ä¸¦åˆ—å‡¦ç†
            random_state=config.RANDOM_STATE,
            callback=[on_step],
            verbose=False
        )
        
        # æœ€é©ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®å–å¾—
        best_n_components = result.x[0]
        best_cov_type = cov_types[result.x[1]]
        best_score = -result.fun
        
        print(f"\n   âœ… ãƒ™ã‚¤ã‚ºæœ€é©åŒ–å®Œäº†")
        print(f"   æœ€é©ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿: n_components={best_n_components}, covariance_type={best_cov_type}")
        print(f"   æœ€è‰¯ã‚¹ã‚³ã‚¢: {best_score:.4f}")
        
        # æœ€é©ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã§æœ€çµ‚ãƒ¢ãƒ‡ãƒ«ã‚’æ§‹ç¯‰
        best_gmm = GaussianMixture(
            n_components=best_n_components,
            covariance_type=best_cov_type,
            random_state=config.RANDOM_STATE,
            n_init=10
        )
        
        best_labels = best_gmm.fit_predict(self.X_scaled)
        
        # ã‚¯ãƒ©ã‚¹ã‚¿åˆ†å¸ƒã‚’è¡¨ç¤º
        unique, counts = np.unique(best_labels, return_counts=True)
        print(f"   ã‚¯ãƒ©ã‚¹ã‚¿æ•°: {len(unique)}")
        for cluster_id, count in zip(unique, counts):
            print(f"   ã‚¯ãƒ©ã‚¹ã‚¿ {cluster_id}: {count}ä»¶")
        
        self.results['GMM'] = {
            'model': best_gmm,
            'labels': best_labels,
            'n_clusters': best_n_components,
            'score': best_score
        }
        
        return best_labels
    
    def try_dbscan(self, eps_range=None, min_samples_range=None, target_clusters=50, use_bayesian=False, n_calls=100):
        """DBSCAN(å¯†åº¦ãƒ™ãƒ¼ã‚¹ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°)
        
        Parameters:
        -----------
        eps_range : list, optional
            æ¢ç´¢ã™ã‚‹epsã®ç¯„å›²ï¼ˆã‚°ãƒªãƒƒãƒ‰ã‚µãƒ¼ãƒã®å ´åˆï¼‰
        min_samples_range : list, optional
            æ¢ç´¢ã™ã‚‹min_samplesã®ç¯„å›²ï¼ˆã‚°ãƒªãƒƒãƒ‰ã‚µãƒ¼ãƒã®å ´åˆï¼‰
        target_clusters : int, default=50
            ç›®æ¨™ã‚¯ãƒ©ã‚¹ã‚¿æ•°ï¼ˆå‚è€ƒå€¤ï¼‰
        use_bayesian : bool, default=False
            Trueã®å ´åˆã€ãƒ™ã‚¤ã‚ºæœ€é©åŒ–ã‚’ä½¿ç”¨
        n_calls : int, default=100
            ãƒ™ã‚¤ã‚ºæœ€é©åŒ–ã®è©•ä¾¡å›æ•°
        """
        print(f"\nğŸŸ¢ DBSCAN (Density-Based Spatial Clustering) ã‚’å®Ÿè¡Œä¸­...")
        print(f"   ç›®æ¨™ã‚¯ãƒ©ã‚¹ã‚¿æ•°: {target_clusters}ç¨‹åº¦")
        
        # ãƒ™ã‚¤ã‚ºæœ€é©åŒ–ã‚’è©¦è¡Œ
        if use_bayesian:
            try:
                from skopt import gp_minimize
                from skopt.space import Real, Integer
                from skopt.utils import use_named_args
                print(f"   ğŸ”¬ ãƒ™ã‚¤ã‚ºæœ€é©åŒ–ã§ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ¢ç´¢ã‚’å®Ÿè¡Œã—ã¾ã™ï¼ˆ{n_calls}å›è©•ä¾¡ï¼‰")
                return self._try_dbscan_bayesian(target_clusters=target_clusters, n_calls=n_calls)
            except ImportError:
                print("   âš ï¸ scikit-optimizeãŒåˆ©ç”¨ã§ãã¾ã›ã‚“ã€‚ã‚°ãƒªãƒƒãƒ‰ã‚µãƒ¼ãƒã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ã—ã¾ã™ã€‚")
        
        # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ç¯„å›²ï¼ˆã‚¯ãƒ©ã‚¹ã‚¿æ•°50ç¨‹åº¦ã«èª¿æ•´ï¼‰
        if eps_range is None:
            eps_range = [0.8, 1.0, 1.2, 1.4, 1.6]
        
        if min_samples_range is None:
            min_samples_range = [15, 20, 25, 30, 35]
        
        best_dbscan = None
        best_labels = None
        best_score = -1
        best_params = None
        
        for eps in eps_range:
            for min_samples in min_samples_range:
                dbscan = DBSCAN(eps=eps, min_samples=min_samples)
                labels = dbscan.fit_predict(self.X_scaled)
                
                # ãƒã‚¤ã‚ºãƒã‚¤ãƒ³ãƒˆï¼ˆ-1ï¼‰ã‚’é™¤ã„ãŸã‚¯ãƒ©ã‚¹ã‚¿æ•°
                n_clusters = len(set(labels)) - (1 if -1 in labels else 0)
                n_noise = list(labels).count(-1)
                
                # ã‚¯ãƒ©ã‚¹ã‚¿ãŒ2ã¤ä»¥ä¸Šã‚ã‚‹å ´åˆã®ã¿è©•ä¾¡
                if n_clusters >= 2:
                    # ãƒã‚¤ã‚ºãƒã‚¤ãƒ³ãƒˆã‚’é™¤å¤–ã—ã¦ã‚·ãƒ«ã‚¨ãƒƒãƒˆã‚¹ã‚³ã‚¢ã‚’è¨ˆç®—
                    mask = labels != -1
                    if mask.sum() > 0:
                        score = silhouette_score(self.X_scaled[mask], labels[mask])
                        
                        # ç›®æ¨™ã‚¯ãƒ©ã‚¹ã‚¿æ•°ã‹ã‚‰ã®è·é›¢ã‚’è¨ˆç®—ï¼ˆãƒšãƒŠãƒ«ãƒ†ã‚£ï¼‰
                        cluster_penalty = abs(n_clusters - target_clusters) / target_clusters
                        adjusted_score = score * (1 - cluster_penalty * 0.5)  # ã‚¯ãƒ©ã‚¹ã‚¿æ•°ã®å·®ã«å¿œã˜ã¦ã‚¹ã‚³ã‚¢ã‚’èª¿æ•´
                        
                        print(f"   eps={eps}, min_samples={min_samples}: "
                              f"ã‚¯ãƒ©ã‚¹ã‚¿æ•°={n_clusters}, ãƒã‚¤ã‚º={n_noise}, "
                              f"ã‚¹ã‚³ã‚¢={score:.4f}, èª¿æ•´å¾Œ={adjusted_score:.4f}")
                        
                        # ãƒã‚¤ã‚ºãŒå°‘ãªãã€èª¿æ•´å¾Œã‚¹ã‚³ã‚¢ãŒé«˜ãã€ã‚¯ãƒ©ã‚¹ã‚¿æ•°ãŒç›®æ¨™ã«è¿‘ã„ã‚‚ã®ã‚’é¸æŠ
                        if (adjusted_score > best_score and 
                            n_noise < len(labels) * 0.35 and 
                            20 <= n_clusters <= 100):  # ã‚¯ãƒ©ã‚¹ã‚¿æ•°ã®å¦¥å½“ãªç¯„å›²ï¼ˆ60ã‚’ä¸­å¿ƒã«ï¼‰
                            best_score = adjusted_score
                            best_dbscan = dbscan
                            best_labels = labels
                            best_params = {'eps': eps, 'min_samples': min_samples}
        
        if best_labels is not None:
            n_clusters_final = len(set(best_labels)) - (1 if -1 in best_labels else 0)
            n_noise_final = list(best_labels).count(-1)
            
            print(f"   âœ“ æœ€é©ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿: eps={best_params['eps']}, "
                  f"min_samples={best_params['min_samples']} (èª¿æ•´å¾Œã‚¹ã‚³ã‚¢: {best_score:.4f})")
            print(f"   âœ“ ã‚¯ãƒ©ã‚¹ã‚¿æ•°: {n_clusters_final}, ãƒã‚¤ã‚º: {n_noise_final}ä»¶")
            
            # ã‚¯ãƒ©ã‚¹ã‚¿åˆ†å¸ƒã‚’è¡¨ç¤ºï¼ˆä¸Šä½10ä»¶ã®ã¿ï¼‰
            unique, counts = np.unique(best_labels, return_counts=True)
            sorted_clusters = sorted(zip(unique, counts), key=lambda x: x[1], reverse=True)
            print(f"   ä¸»è¦ã‚¯ãƒ©ã‚¹ã‚¿åˆ†å¸ƒï¼ˆä¸Šä½10ä»¶ï¼‰:")
            for i, (cluster_id, count) in enumerate(sorted_clusters[:10]):
                label_name = "ãƒã‚¤ã‚º" if cluster_id == -1 else f"ã‚¯ãƒ©ã‚¹ã‚¿ {cluster_id}"
                print(f"     {label_name}: {count}ä»¶")
            
            self.results['DBSCAN'] = {
                'model': best_dbscan,
                'labels': best_labels,
                'n_clusters': len(set(best_labels)) - (1 if -1 in best_labels else 0),
                'score': best_score,
                'params': best_params
            }
        else:
            print(f"   âš ï¸ é©åˆ‡ãªDBSCANãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")
            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã§å®Ÿè¡Œ
            dbscan = DBSCAN(eps=0.5, min_samples=5)
            best_labels = dbscan.fit_predict(self.X_scaled)
            
            self.results['DBSCAN'] = {
                'model': dbscan,
                'labels': best_labels,
                'n_clusters': len(set(best_labels)) - (1 if -1 in best_labels else 0),
                'score': -1,
                'params': {'eps': 0.5, 'min_samples': 5}
            }
        
        return best_labels
    
    def _try_dbscan_bayesian(self, target_clusters=50, n_calls=100):
        """DBSCANã®ãƒ™ã‚¤ã‚ºæœ€é©åŒ–
        
        Parameters:
        -----------
        target_clusters : int
            ç›®æ¨™ã‚¯ãƒ©ã‚¹ã‚¿æ•°ï¼ˆå‚è€ƒå€¤ï¼‰
        n_calls : int
            ãƒ™ã‚¤ã‚ºæœ€é©åŒ–ã®è©•ä¾¡å›æ•°
        """
        from skopt import forest_minimize  # ä¸¦åˆ—åŒ–ã«é©ã—ãŸæ‰‹æ³•
        from skopt.space import Real, Integer
        from skopt.utils import use_named_args
        from sklearn.metrics import davies_bouldin_score
        
        # æ¢ç´¢ç©ºé–“ã®å®šç¾©ï¼ˆeps: 0.5-2.0, min_samples: 5-50ï¼‰
        space = [
            Real(0.5, 2.0, name='eps'),
            Integer(5, 50, name='min_samples')
        ]
        
        @use_named_args(space)
        def objective(eps, min_samples):
            """æœ€å°åŒ–ã™ã‚‹ç›®çš„é–¢æ•°ï¼ˆè² ã®ã‚¹ã‚³ã‚¢ã‚’è¿”ã™ï¼‰"""
            try:
                # DBSCANãƒ¢ãƒ‡ãƒ«ã®æ§‹ç¯‰ã¨è¨“ç·´
                dbscan = DBSCAN(eps=eps, min_samples=min_samples)
                labels = dbscan.fit_predict(self.X_scaled)
                
                # ãƒã‚¤ã‚ºãƒã‚¤ãƒ³ãƒˆï¼ˆ-1ï¼‰ã‚’é™¤ã„ãŸã‚¯ãƒ©ã‚¹ã‚¿æ•°
                n_clusters = len(set(labels)) - (1 if -1 in labels else 0)
                n_noise = list(labels).count(-1)
                noise_ratio = n_noise / len(labels)
                
                # ã‚¯ãƒ©ã‚¹ã‚¿ãŒ2ã¤æœªæº€ã®å ´åˆã¯ãƒšãƒŠãƒ«ãƒ†ã‚£
                if n_clusters < 2:
                    return 0.0  # æœ€æ‚ªã®ã‚¹ã‚³ã‚¢
                
                # ãƒã‚¤ã‚ºãƒã‚¤ãƒ³ãƒˆã‚’é™¤å¤–ã—ã¦è©•ä¾¡
                mask = labels != -1
                if mask.sum() < 2:
                    return 0.0
                
                # ã‚·ãƒ«ã‚¨ãƒƒãƒˆã‚¹ã‚³ã‚¢ï¼ˆ0-1, é«˜ã„ã»ã©è‰¯ã„ï¼‰
                silhouette = silhouette_score(self.X_scaled[mask], labels[mask])
                
                # Davis-Bouldin Indexï¼ˆ0ä»¥ä¸Šã€ä½ã„ã»ã©è‰¯ã„ â†’ æ­£è¦åŒ–ï¼‰
                db_index = davies_bouldin_score(self.X_scaled[mask], labels[mask])
                db_normalized = max(0, 1.0 - db_index / 3.0)  # 3.0ã§å‰²ã£ã¦0-1ã«æ­£è¦åŒ–
                
                # ã‚¯ãƒ©ã‚¹ã‚¿ãƒãƒ©ãƒ³ã‚¹ã‚¹ã‚³ã‚¢ï¼ˆæ¨™æº–åå·®ãƒ™ãƒ¼ã‚¹ï¼‰
                unique_labels, counts = np.unique(labels[mask], return_counts=True)
                if len(counts) > 1:
                    std_count = counts.std()
                    mean_count = counts.mean()
                    balance_score = max(0, 1.0 - std_count / (mean_count + 1e-6))
                else:
                    balance_score = 0.0
                
                # è¤‡åˆã‚¹ã‚³ã‚¢ (0.35:0.35:0.3 = Silhouette:DB:Balance)
                combined_score = 0.35 * silhouette + 0.35 * db_normalized + 0.3 * balance_score
                
                # ã‚¯ãƒ©ã‚¹ã‚¿æ•°ãƒšãƒŠãƒ«ãƒ†ã‚£ï¼ˆ10ä»¥ä¸Šã€76ä»¥ä¸‹ã‚’å¼·åˆ¶ï¼‰
                if n_clusters < 10:
                    combined_score *= 0.1  # 90%ãƒšãƒŠãƒ«ãƒ†ã‚£
                elif n_clusters < 15:
                    combined_score *= 0.6  # 40%ãƒšãƒŠãƒ«ãƒ†ã‚£
                elif n_clusters > 76:
                    combined_score *= 0.1  # 90%ãƒšãƒŠãƒ«ãƒ†ã‚£
                elif n_clusters > 60:
                    combined_score *= 0.7  # 30%ãƒšãƒŠãƒ«ãƒ†ã‚£
                
                # ãƒã‚¤ã‚ºæ¯”ç‡ãƒšãƒŠãƒ«ãƒ†ã‚£ï¼ˆ35%ä»¥ä¸Šã¯å¤§å¹…æ¸›ç‚¹ï¼‰
                if noise_ratio < 0.10:
                    combined_score *= (1.0 - noise_ratio * 0.5)
                elif noise_ratio < 0.35:
                    combined_score *= max(0.1, 1.0 - noise_ratio * 1.5)
                else:
                    combined_score *= 0.05  # 95%ãƒšãƒŠãƒ«ãƒ†ã‚£
                
                return -combined_score  # æœ€å°åŒ–ã™ã‚‹ãŸã‚è² ã®å€¤ã‚’è¿”ã™
                
            except Exception as e:
                print(f"   âš ï¸ DBSCANè©•ä¾¡ã‚¨ãƒ©ãƒ¼ (eps={eps:.2f}, min_samples={min_samples}): {e}")
                return 0.0  # ã‚¨ãƒ©ãƒ¼æ™‚ã¯æœ€æ‚ªã®ã‚¹ã‚³ã‚¢
        
        # é€²æ—è¡¨ç¤ºç”¨ã®ã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯
        iteration = [0]
        def on_step(res):
            iteration[0] += 1
            if iteration[0] % 10 == 0 or iteration[0] <= 5:
                print(f"   è©•ä¾¡ {iteration[0]}/{n_calls}: ç¾åœ¨ã®æœ€è‰¯ã‚¹ã‚³ã‚¢ = {-res.fun:.4f}")
        
        # ãƒ™ã‚¤ã‚ºæœ€é©åŒ–å®Ÿè¡Œï¼ˆforest_minimizeã§ä¸¦åˆ—å‡¦ç†æœ‰åŠ¹ï¼‰
        cores_msg = "å…¨CPUã‚³ã‚¢" if config.N_JOBS == -1 else f"{config.N_JOBS}ã‚³ã‚¢"
        print(f"   ğŸš€ Random Forestãƒ™ãƒ¼ã‚¹ã§{cores_msg}ä¸¦åˆ—å‡¦ç†ã‚’å®Ÿè¡Œ...")
        print(f"   ğŸ“Š åˆæœŸãƒ©ãƒ³ãƒ€ãƒ æ¢ç´¢: {config.N_INITIAL_POINTS}å›(ä¸¦åˆ—)ã€é€æ¬¡æ¢ç´¢: {n_calls - config.N_INITIAL_POINTS}å›")
        result = forest_minimize(
            objective,
            space,
            n_calls=n_calls,
            n_initial_points=config.N_INITIAL_POINTS,
            n_jobs=config.N_JOBS,  # ä¸¦åˆ—å‡¦ç†
            random_state=config.RANDOM_STATE,
            callback=[on_step],
            verbose=False
        )
        
        # æœ€é©ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®å–å¾—
        best_eps = result.x[0]
        best_min_samples = result.x[1]
        best_score = -result.fun
        
        print(f"\n   âœ… ãƒ™ã‚¤ã‚ºæœ€é©åŒ–å®Œäº†")
        print(f"   æœ€é©ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿: eps={best_eps:.3f}, min_samples={best_min_samples}")
        print(f"   æœ€è‰¯ã‚¹ã‚³ã‚¢: {best_score:.4f}")
        
        # æœ€é©ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã§æœ€çµ‚ãƒ¢ãƒ‡ãƒ«ã‚’æ§‹ç¯‰
        best_dbscan = DBSCAN(eps=best_eps, min_samples=best_min_samples)
        best_labels = best_dbscan.fit_predict(self.X_scaled)
        
        # ã‚¯ãƒ©ã‚¹ã‚¿æƒ…å ±ã‚’è¡¨ç¤º
        n_clusters = len(set(best_labels)) - (1 if -1 in best_labels else 0)
        n_noise = list(best_labels).count(-1)
        noise_ratio = n_noise / len(best_labels)
        
        print(f"   ã‚¯ãƒ©ã‚¹ã‚¿æ•°: {n_clusters}, ãƒã‚¤ã‚º: {n_noise}ä»¶ ({noise_ratio*100:.1f}%)")
        
        # ã‚¯ãƒ©ã‚¹ã‚¿åˆ†å¸ƒã‚’è¡¨ç¤ºï¼ˆä¸Šä½10ä»¶ã®ã¿ï¼‰
        unique, counts = np.unique(best_labels, return_counts=True)
        sorted_clusters = sorted(zip(unique, counts), key=lambda x: x[1], reverse=True)
        print(f"   ä¸»è¦ã‚¯ãƒ©ã‚¹ã‚¿åˆ†å¸ƒï¼ˆä¸Šä½10ä»¶ï¼‰:")
        for i, (cluster_id, count) in enumerate(sorted_clusters[:10]):
            label_name = "ãƒã‚¤ã‚º" if cluster_id == -1 else f"ã‚¯ãƒ©ã‚¹ã‚¿ {cluster_id}"
            print(f"     {label_name}: {count}ä»¶")
        
        self.results['DBSCAN'] = {
            'model': best_dbscan,
            'labels': best_labels,
            'n_clusters': n_clusters,
            'score': best_score,
            'params': {'eps': best_eps, 'min_samples': best_min_samples}
        }
        
        return best_labels
    
    def try_hdbscan(self, min_cluster_size_range=None, min_samples_range=None, target_clusters=50, use_bayesian=True, n_calls=100):
        """HDBSCAN (Hierarchical Density-Based Spatial Clustering) with Bayesian Optimization
        
        DBSCANã®ã‚¯ãƒ©ã‚¹ã‚¿æ•°ãŒå¤šã™ãã‚‹å ´åˆã®ä»£æ›¿æ‰‹æ³•ã¨ã—ã¦ä½¿ç”¨ã€‚
        HDBSCANã¯éšå±¤çš„ãªå¯†åº¦ãƒ™ãƒ¼ã‚¹ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ã§ã€ã‚ˆã‚Šé©å¿œçš„ãªã‚¯ãƒ©ã‚¹ã‚¿ã‚’ç”Ÿæˆã™ã‚‹ã€‚
        
        Args:
            use_bayesian: ãƒ™ã‚¤ã‚ºæœ€é©åŒ–ã‚’ä½¿ç”¨ã™ã‚‹ã‹ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: Trueï¼‰
            n_calls: ãƒ™ã‚¤ã‚ºæœ€é©åŒ–ã®è©•ä¾¡å›æ•°ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 100ï¼‰
        """
        print(f"\nğŸŸ¡ HDBSCAN (Hierarchical DBSCAN) ã‚’å®Ÿè¡Œä¸­...")
        print(f"   ç›®æ¨™ã‚¯ãƒ©ã‚¹ã‚¿æ•°: {target_clusters}ç¨‹åº¦")
        if use_bayesian:
            print(f"   æœ€é©åŒ–æ‰‹æ³•: ãƒ™ã‚¤ã‚ºæœ€é©åŒ–ï¼ˆè©•ä¾¡å›æ•°: {n_calls}å›ï¼‰")
        
        try:
            import hdbscan
        except ImportError:
            print(f"   âš ï¸ HDBSCANãŒã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚")
            print(f"   'pip install hdbscan' ã§ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã—ã¦ãã ã•ã„ã€‚")
            return None
        
        # ãƒ™ã‚¤ã‚ºæœ€é©åŒ–ã‚’ä½¿ç”¨ã™ã‚‹å ´åˆ
        if use_bayesian:
            try:
                from skopt import gp_minimize
                from skopt.space import Integer
                from skopt.utils import use_named_args
            except ImportError:
                print(f"   âš ï¸ scikit-optimizeãŒã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚")
                print(f"   'pip install scikit-optimize' ã§ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã™ã‚‹ã‹ã€use_bayesian=Falseã«è¨­å®šã—ã¦ãã ã•ã„ã€‚")
                print(f"   ã‚°ãƒªãƒƒãƒ‰ã‚µãƒ¼ãƒã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ã—ã¾ã™...")
                use_bayesian = False
        
        if use_bayesian:
            return self._try_hdbscan_bayesian(target_clusters, n_calls)
        
        # ã‚°ãƒªãƒƒãƒ‰ã‚µãƒ¼ãƒï¼ˆå¾“æ¥ã®æ–¹æ³•ï¼‰
        # v0.8æ”¹: ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ¢ç´¢å¯†åº¦ã‚’2å€ã«å¢—åŠ ï¼ˆv0.6ã®13å€‹â†’26å€‹ï¼‰
        if min_cluster_size_range is None:
            # æ¢ç´¢å¯†åº¦2å€: 3-100ã‚’26å€‹ã®ãƒã‚¤ãƒ³ãƒˆã§æ¢ç´¢
            min_cluster_size_range = [3, 5, 8, 10, 15, 20, 25, 30, 40, 50, 60, 80, 100]
        
        if min_samples_range is None:
            # æ¢ç´¢å¯†åº¦2å€: 1-25ã‚’18å€‹ã®ãƒã‚¤ãƒ³ãƒˆã§æ¢ç´¢
            min_samples_range = [1, 2, 3, 5, 8, 10, 15, 20, 25]
        
        best_hdbscan = None
        best_labels = None
        best_score = -1
        best_params = None
        
        for min_cluster_size in min_cluster_size_range:
            for min_samples in min_samples_range:
                try:
                    clusterer = hdbscan.HDBSCAN(
                        min_cluster_size=min_cluster_size,
                        min_samples=min_samples,
                        cluster_selection_method='eom',  # Excess of Mass
                        metric='euclidean'
                    )
                    
                    labels = clusterer.fit_predict(self.X_scaled)
                    
                    # ã‚¯ãƒ©ã‚¹ã‚¿æ•°ã¨ãƒã‚¤ã‚ºãƒã‚¤ãƒ³ãƒˆæ•°
                    n_clusters = len(set(labels)) - (1 if -1 in labels else 0)
                    n_noise = list(labels).count(-1)
                    
                    # ã‚¯ãƒ©ã‚¹ã‚¿ãŒ2ã¤ä»¥ä¸Šã‚ã‚‹å ´åˆã®ã¿è©•ä¾¡
                    if n_clusters >= 2:
                        # ãƒã‚¤ã‚ºãƒã‚¤ãƒ³ãƒˆã‚’é™¤å¤–ã—ã¦ã‚·ãƒ«ã‚¨ãƒƒãƒˆã‚¹ã‚³ã‚¢ã‚’è¨ˆç®—
                        mask = labels != -1
                        if mask.sum() > 1 and len(set(labels[mask])) > 1:
                            score = silhouette_score(self.X_scaled[mask], labels[mask])
                            
                            # ç›®æ¨™ã‚¯ãƒ©ã‚¹ã‚¿æ•°ã‹ã‚‰ã®è·é›¢ã‚’è¨ˆç®—(ãƒšãƒŠãƒ«ãƒ†ã‚£)
                            cluster_penalty = abs(n_clusters - target_clusters) / target_clusters
                            # v0.7æ”¹: ãƒã‚¤ã‚º10%æœªæº€ã‚’æœ€å„ªå…ˆç›®æ¨™ã¨ã™ã‚‹
                            noise_ratio = n_noise / len(labels)
                            
                            # ãƒã‚¤ã‚º10%æœªæº€ãªã‚‰å¤§å¹…ãƒœãƒ¼ãƒŠã‚¹ã€ãã‚Œä»¥ä¸Šã¯å¤§å¹…ãƒšãƒŠãƒ«ãƒ†ã‚£
                            if noise_ratio < 0.10:
                                noise_bonus = 1.0 + (0.10 - noise_ratio) * 10.0  # 10%æœªæº€ãªã‚‰æœ€å¤§+100%ãƒœãƒ¼ãƒŠã‚¹
                            else:
                                noise_bonus = 1.0 - (noise_ratio - 0.10) * 5.0  # 10%è¶…éã”ã¨ã«-50%ãƒšãƒŠãƒ«ãƒ†ã‚£
                            
                            adjusted_score = score * (1 - cluster_penalty * 0.2) * max(0.01, noise_bonus)
                            
                            print(f"   min_cluster_size={min_cluster_size}, min_samples={min_samples}: "
                                  f"ã‚¯ãƒ©ã‚¹ã‚¿æ•°={n_clusters}, ãƒã‚¤ã‚º={n_noise}({noise_ratio*100:.1f}%), "
                                  f"ã‚¹ã‚³ã‚¢={score:.4f}, èª¿æ•´å¾Œ={adjusted_score:.4f}")
                            
                            # v0.7æ”¹: ãƒã‚¤ã‚ºé–¾å€¤ã‚’æ’¤å»ƒï¼ˆèª¿æ•´å¾Œã‚¹ã‚³ã‚¢ã§è‡ªå‹•çš„ã«10%æœªæº€ãŒå„ªé‡ã•ã‚Œã‚‹ï¼‰
                            # ã‚¯ãƒ©ã‚¹ã‚¿æ•°ç¯„å›²: 10ã€œ150å€‹ï¼ˆã‚ˆã‚Šåºƒãè¨±å®¹ï¼‰
                            if (adjusted_score > best_score and 
                                10 <= n_clusters <= 150):
                                best_score = adjusted_score
                                best_hdbscan = clusterer
                                best_labels = labels
                                best_params = {'min_cluster_size': min_cluster_size, 'min_samples': min_samples}
                
                except Exception as e:
                    print(f"   âš ï¸ min_cluster_size={min_cluster_size}, min_samples={min_samples}ã§ã‚¨ãƒ©ãƒ¼: {e}")
                    continue
        
        # ãƒã‚¤ã‚º15%ä»¥ä¸‹ã®å€™è£œãŒãªã„å ´åˆã€æœ€è‰¯ã®çµæœã‚’æ¡ç”¨
        if best_labels is None:
            print(f"   âš ï¸ ãƒã‚¤ã‚º15%ä»¥ä¸‹ã®å€™è£œãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚æœ€è‰¯ã®çµæœã‚’æ¡ç”¨ã—ã¾ã™ã€‚")
            # æœ€å°ãƒã‚¤ã‚ºã®çµæœã‚’å†æ¢ç´¢
            for min_cluster_size in min_cluster_size_range:
                for min_samples in min_samples_range:
                    try:
                        clusterer = hdbscan.HDBSCAN(
                            min_cluster_size=min_cluster_size,
                            min_samples=min_samples,
                            metric='euclidean'
                        )
                        labels = clusterer.fit_predict(self.X_scaled)
                        n_clusters = len(set(labels)) - (1 if -1 in labels else 0)
                        n_noise = list(labels).count(-1)
                        
                        if n_clusters >= 2:
                            mask = labels != -1
                            if mask.sum() > 1 and len(set(labels[mask])) > 1:
                                score = silhouette_score(self.X_scaled[mask], labels[mask])
                                cluster_penalty = abs(n_clusters - target_clusters) / target_clusters
                                noise_ratio = n_noise / len(labels)
                                noise_penalty = noise_ratio
                                adjusted_score = score * (1 - cluster_penalty * 0.5) * (1 - noise_penalty * 0.6)
                                
                                if (adjusted_score > best_score and 
                                    n_noise < len(labels) * 0.30 and  # ç·©å’Œã—ãŸåŸºæº–
                                    20 <= n_clusters <= 80):
                                    best_score = adjusted_score
                                    best_hdbscan = clusterer
                                    best_labels = labels
                                    best_params = {'min_cluster_size': min_cluster_size, 'min_samples': min_samples}
                    except:
                        continue
        
        if best_labels is not None:
            n_clusters_final = len(set(best_labels)) - (1 if -1 in best_labels else 0)
            n_noise_final = list(best_labels).count(-1)
            noise_ratio_final = n_noise_final / len(best_labels)
            
            print(f"   âœ“ æœ€é©ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿: min_cluster_size={best_params['min_cluster_size']}, "
                  f"min_samples={best_params['min_samples']} (èª¿æ•´å¾Œã‚¹ã‚³ã‚¢: {best_score:.4f})")
            print(f"   âœ“ ã‚¯ãƒ©ã‚¹ã‚¿æ•°: {n_clusters_final}, ãƒã‚¤ã‚º: {n_noise_final}ä»¶ ({noise_ratio_final*100:.1f}%)")
            
            # ã‚¯ãƒ©ã‚¹ã‚¿åˆ†å¸ƒã‚’è¡¨ç¤ºï¼ˆä¸Šä½10ä»¶ã®ã¿ï¼‰
            unique, counts = np.unique(best_labels, return_counts=True)
            sorted_clusters = sorted(zip(unique, counts), key=lambda x: x[1], reverse=True)
            print(f"   ä¸»è¦ã‚¯ãƒ©ã‚¹ã‚¿åˆ†å¸ƒï¼ˆä¸Šä½10ä»¶ï¼‰:")
            for i, (cluster_id, count) in enumerate(sorted_clusters[:10]):
                label_name = "ãƒã‚¤ã‚º" if cluster_id == -1 else f"ã‚¯ãƒ©ã‚¹ã‚¿ {cluster_id}"
                print(f"     {label_name}: {count}ä»¶")
            
            self.results['HDBSCAN'] = {
                'model': best_hdbscan,
                'labels': best_labels,
                'n_clusters': n_clusters_final,
                'score': best_score,
                'params': best_params
            }
        else:
            print(f"   âš ï¸ é©åˆ‡ãªHDBSCANãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")
            self.results['HDBSCAN'] = None
        
        return best_labels
    
    def _try_hdbscan_bayesian(self, target_clusters, n_calls):
        """ãƒ™ã‚¤ã‚ºæœ€é©åŒ–ã‚’ä½¿ç”¨ã—ãŸHDBSCANãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ¢ç´¢
        
        Note: target_clustersã¯äº’æ›æ€§ã®ãŸã‚ã«æ®‹ã—ã¦ã„ã‚‹ãŒã€æœ€é©åŒ–ã§ã¯ä½¿ç”¨ã—ãªã„
        """
        import hdbscan
        from skopt import forest_minimize  # ä¸¦åˆ—åŒ–ã«é©ã—ãŸæ‰‹æ³•
        from skopt.space import Integer
        from skopt.utils import use_named_args
        
        # æ¢ç´¢ç©ºé–“ã®å®šç¾©ï¼ˆv0.8æ”¹: ã‚ˆã‚Šé©åˆ‡ãªç¯„å›²ã«çµã‚‹ï¼‰
        space = [
            Integer(10, 100, name='min_cluster_size'),  # ä¸‹é™ã‚’10ã«å¼•ãä¸Šã’
            Integer(3, 25, name='min_samples')  # ä¸‹é™ã‚’3ã«å¼•ãä¸Šã’
        ]
        
        # ç›®çš„é–¢æ•°ï¼ˆæœ€å°åŒ–ã™ã‚‹ãŸã‚ã€èª¿æ•´å¾Œã‚¹ã‚³ã‚¢ã®è² å€¤ã‚’è¿”ã™ï¼‰
        @use_named_args(space)
        def objective(min_cluster_size, min_samples):
            try:
                clusterer = hdbscan.HDBSCAN(
                    min_cluster_size=min_cluster_size,
                    min_samples=min_samples,
                    cluster_selection_method='eom',
                    metric='euclidean'
                )
                
                labels = clusterer.fit_predict(self.X_scaled)
                
                # ã‚¯ãƒ©ã‚¹ã‚¿æ•°è¨ˆç®—
                n_clusters = len(set(labels)) - (1 if -1 in labels else 0)
                n_noise = list(labels).count(-1)
                noise_ratio = n_noise / len(labels)
                
                # åˆ¶ç´„ãƒã‚§ãƒƒã‚¯
                if n_clusters < 2:
                    return 10.0  # ãƒšãƒŠãƒ«ãƒ†ã‚£å¤§
                
                # ãƒã‚¤ã‚ºãƒã‚¤ãƒ³ãƒˆã‚’é™¤å¤–ã—ã¦ã‚·ãƒ«ã‚¨ãƒƒãƒˆã‚¹ã‚³ã‚¢ã‚’è¨ˆç®—
                mask = labels != -1
                if mask.sum() <= 1 or len(set(labels[mask])) <= 1:
                    return 10.0
                
                # v0.8æ”¹: Silhouetteä¿‚æ•°ã¨Davis-Bouldin Indexã‚’0.5:0.5ã§è©•ä¾¡
                from sklearn.metrics import davies_bouldin_score
                
                silhouette = silhouette_score(self.X_scaled[mask], labels[mask])
                db_index = davies_bouldin_score(self.X_scaled[mask], labels[mask])
                
                # Silhouette: é«˜ã„æ–¹ãŒè‰¯ã„ (0-1), DB Index: ä½ã„æ–¹ãŒè‰¯ã„ (0-)
                # DB Indexã‚’0-1ã«æ­£è¦åŒ–ï¼ˆå…¸å‹çš„ãªç¯„å›²0-3ã‚’æƒ³å®šï¼‰
                db_normalized = max(0, 1.0 - db_index / 3.0)
                
                # ã‚¯ãƒ©ã‚¹ã‚¿ãƒãƒ©ãƒ³ã‚¹ã‚¹ã‚³ã‚¢ï¼ˆå„ã‚¯ãƒ©ã‚¹ã‚¿ã®å‡ç­‰æ€§ã‚’è©•ä¾¡ï¼‰
                unique_labels, counts = np.unique(labels[mask], return_counts=True)
                if len(counts) > 1:
                    # æ¨™æº–åå·®ã‚’ä½¿ç”¨ã—ã¦ä¸å‡è¡¡åº¦ã‚’è¨ˆç®—ï¼ˆ0-1ã«æ­£è¦åŒ–ï¼‰
                    mean_count = counts.mean()
                    std_count = counts.std()
                    balance_score = max(0, 1.0 - std_count / (mean_count + 1e-6))
                else:
                    balance_score = 0.0
                
                # è¤‡åˆã‚¹ã‚³ã‚¢ (0.35:0.35:0.3 = Silhouette:DB:Balance)
                # v0.8æ”¹: ãƒãƒ©ãƒ³ã‚¹ã‚¹ã‚³ã‚¢ã®é‡ã¿ã‚’0.2â†’0.3ã«å¢—åŠ 
                combined_score = 0.35 * silhouette + 0.35 * db_normalized + 0.3 * balance_score
                
                # ã‚¯ãƒ©ã‚¹ã‚¿æ•°ãƒšãƒŠãƒ«ãƒ†ã‚£ï¼ˆ10ä»¥ä¸Šã€76ä»¥ä¸‹ã‚’å¼·åˆ¶ã€15-76ãŒç†æƒ³ç¯„å›²ï¼‰
                # 76 = è‡ªæ²»ä½“ç·æ•°19 Ã— 4
                if n_clusters < 10:
                    combined_score *= 0.1  # 90%æ¸›ç‚¹ï¼ˆéå¸¸ã«å¤§ããªãƒšãƒŠãƒ«ãƒ†ã‚£ï¼‰
                elif n_clusters < 15:
                    combined_score *= 0.6  # 40%æ¸›ç‚¹
                elif n_clusters > 76:
                    combined_score *= 0.1  # 90%æ¸›ç‚¹ï¼ˆä¸Šé™è¶…éã‚‚å³ã—ããƒšãƒŠãƒ«ãƒ†ã‚£ï¼‰
                elif n_clusters > 60:
                    combined_score *= 0.7  # 30%æ¸›ç‚¹
                
                # ãƒã‚¤ã‚ºãƒšãƒŠãƒ«ãƒ†ã‚£ï¼ˆ10%æœªæº€ã¯è»½å¾®ã€10%ä»¥ä¸Šã¯å¤§ããªãƒšãƒŠãƒ«ãƒ†ã‚£ï¼‰
                if noise_ratio < 0.10:
                    combined_score *= (1.0 - noise_ratio * 0.5)  # æœ€å¤§5%æ¸›ç‚¹
                else:
                    combined_score *= max(0.01, 1.0 - noise_ratio * 2.0)  # å¤§å¹…æ¸›ç‚¹
                
                return -combined_score  # æœ€å°åŒ–ã™ã‚‹ãŸã‚è² å€¤
                
            except Exception as e:
                return 10.0  # ã‚¨ãƒ©ãƒ¼æ™‚ã¯å¤§ããªãƒšãƒŠãƒ«ãƒ†ã‚£
        
        # ãƒ™ã‚¤ã‚ºæœ€é©åŒ–ã®å®Ÿè¡Œï¼ˆé€²æ—è¡¨ç¤ºä»˜ãï¼‰
        print(f"   ãƒ™ã‚¤ã‚ºæœ€é©åŒ–ã‚’é–‹å§‹ï¼ˆ{n_calls}å›ã®è©•ä¾¡ï¼‰...")
        
        # ã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯é–¢æ•°ã§é€²æ—è¡¨ç¤º
        iteration = [0]
        def on_step(res):
            iteration[0] += 1
            if iteration[0] % 10 == 0 or iteration[0] <= 5:
                best_score = -res.fun
                print(f"      è©•ä¾¡ {iteration[0]}/{n_calls}: ç¾åœ¨ã®æœ€è‰¯ã‚¹ã‚³ã‚¢ = {best_score:.4f}")
        
        # ãƒ™ã‚¤ã‚ºæœ€é©åŒ–å®Ÿè¡Œï¼ˆforest_minimizeã§ä¸¦åˆ—å‡¦ç†æœ‰åŠ¹ï¼‰
        cores_msg = "å…¨CPUã‚³ã‚¢" if config.N_JOBS == -1 else f"{config.N_JOBS}ã‚³ã‚¢"
        print(f"   ğŸš€ Random Forestãƒ™ãƒ¼ã‚¹ã§{cores_msg}ä¸¦åˆ—å‡¦ç†ã‚’å®Ÿè¡Œ...")
        print(f"   ğŸ“Š åˆæœŸãƒ©ãƒ³ãƒ€ãƒ æ¢ç´¢: {config.N_INITIAL_POINTS}å›(ä¸¦åˆ—)ã€é€æ¬¡æ¢ç´¢: {n_calls - config.N_INITIAL_POINTS}å›")
        result = forest_minimize(
            objective,
            space,
            n_calls=n_calls,
            random_state=config.RANDOM_STATE,
            verbose=False,
            n_initial_points=config.N_INITIAL_POINTS,
            n_jobs=config.N_JOBS,  # ä¸¦åˆ—å‡¦ç†
            callback=[on_step]
        )
        
        # æœ€é©ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã§æœ€çµ‚ãƒ¢ãƒ‡ãƒ«ã‚’æ§‹ç¯‰
        best_min_cluster_size = result.x[0]
        best_min_samples = result.x[1]
        best_score = -result.fun
        
        print(f"   âœ“ æœ€é©ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿: min_cluster_size={best_min_cluster_size}, min_samples={best_min_samples}")
        print(f"   âœ“ èª¿æ•´å¾Œã‚¹ã‚³ã‚¢: {best_score:.4f}")
        
        # æœ€é©ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã§æœ€çµ‚ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°
        best_hdbscan = hdbscan.HDBSCAN(
            min_cluster_size=best_min_cluster_size,
            min_samples=best_min_samples,
            cluster_selection_method='eom',
            metric='euclidean'
        )
        best_labels = best_hdbscan.fit_predict(self.X_scaled)
        
        n_clusters_final = len(set(best_labels)) - (1 if -1 in best_labels else 0)
        n_noise_final = list(best_labels).count(-1)
        noise_ratio_final = n_noise_final / len(best_labels)
        
        print(f"   âœ“ ã‚¯ãƒ©ã‚¹ã‚¿æ•°: {n_clusters_final}, ãƒã‚¤ã‚º: {n_noise_final}ä»¶ ({noise_ratio_final*100:.1f}%)")
        
        # ã‚¯ãƒ©ã‚¹ã‚¿åˆ†å¸ƒã‚’è¡¨ç¤ºï¼ˆä¸Šä½10ä»¶ã®ã¿ï¼‰
        unique, counts = np.unique(best_labels, return_counts=True)
        sorted_clusters = sorted(zip(unique, counts), key=lambda x: x[1], reverse=True)
        print(f"   ä¸»è¦ã‚¯ãƒ©ã‚¹ã‚¿åˆ†å¸ƒï¼ˆä¸Šä½10ä»¶ï¼‰:")
        for i, (cluster_id, count) in enumerate(sorted_clusters[:10]):
            label_name = "ãƒã‚¤ã‚º" if cluster_id == -1 else f"ã‚¯ãƒ©ã‚¹ã‚¿ {cluster_id}"
            print(f"     {label_name}: {count}ä»¶")
        
        self.results['HDBSCAN'] = {
            'model': best_hdbscan,
            'labels': best_labels,
            'n_clusters': n_clusters_final,
            'score': best_score,
            'params': {'min_cluster_size': best_min_cluster_size, 'min_samples': best_min_samples}
        }
        
        return best_labels
    
    def try_classix(self, radius_range=None, min_samples_range=None, target_clusters=28, use_bayesian=True, n_calls=100):
        """CLASSIX (Fast and Explainable Clustering) with Bayesian Optimization
        
        HDBSCANã®ãƒã‚¤ã‚ºå•é¡Œã‚’è§£æ±ºã™ã‚‹ä»£æ›¿æ‰‹æ³•ã€‚
        CLASSIXã¯é«˜é€Ÿã§ã€ãƒã‚¤ã‚ºãŒå°‘ãªãã€èª¬æ˜å¯èƒ½ãªã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ã‚’å®Ÿç¾ã™ã‚‹ã€‚
        
        Args:
            use_bayesian: ãƒ™ã‚¤ã‚ºæœ€é©åŒ–ã‚’ä½¿ç”¨ã™ã‚‹ã‹ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: Trueï¼‰
            n_calls: ãƒ™ã‚¤ã‚ºæœ€é©åŒ–ã®è©•ä¾¡å›æ•°ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 100ï¼‰
        """
        print(f"\nğŸŸ  CLASSIX (Fast Clustering) ã‚’å®Ÿè¡Œä¸­...")
        print(f"   ç›®æ¨™ã‚¯ãƒ©ã‚¹ã‚¿æ•°: {target_clusters}ç¨‹åº¦")
        if use_bayesian:
            print(f"   æœ€é©åŒ–æ‰‹æ³•: ãƒ™ã‚¤ã‚ºæœ€é©åŒ–ï¼ˆè©•ä¾¡å›æ•°: {n_calls}å›ï¼‰")
        
        try:
            from classix import CLASSIX
        except ImportError:
            print(f"   âš ï¸ CLASSIXãŒã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚")
            print(f"   'pip install classixclustering' ã§ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã—ã¦ãã ã•ã„ã€‚")
            return None
        
        # ãƒ™ã‚¤ã‚ºæœ€é©åŒ–ã‚’ä½¿ç”¨ã™ã‚‹å ´åˆ
        if use_bayesian:
            try:
                from skopt import gp_minimize
                from skopt.space import Real, Integer
                from skopt.utils import use_named_args
            except ImportError:
                print(f"   âš ï¸ scikit-optimizeãŒã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚")
                print(f"   'pip install scikit-optimize' ã§ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã™ã‚‹ã‹ã€use_bayesian=Falseã«è¨­å®šã—ã¦ãã ã•ã„ã€‚")
                print(f"   ã‚°ãƒªãƒƒãƒ‰ã‚µãƒ¼ãƒã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ã—ã¾ã™...")
                use_bayesian = False
        
        if use_bayesian:
            return self._try_classix_bayesian(target_clusters, n_calls)
        
        # ã‚°ãƒªãƒƒãƒ‰ã‚µãƒ¼ãƒï¼ˆå¾“æ¥ã®æ–¹æ³•ï¼‰
        # v0.8æ”¹: ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ¢ç´¢å¯†åº¦ã‚’8å€ã«å¢—åŠ ï¼ˆã‚ˆã‚Šç´°ã‹ãæœ€é©åŒ–ï¼‰
        if radius_range is None:
            # æ¢ç´¢å¯†åº¦8å€: 0.05-1.0ã‚’56å€‹ã®ãƒã‚¤ãƒ³ãƒˆã§æ¢ç´¢
            radius_range = [round(r, 3) for r in list(np.arange(0.05, 0.31, 0.02)) + list(np.arange(0.31, 0.61, 0.03)) + list(np.arange(0.61, 1.01, 0.05))]
        
        if min_samples_range is None:
            # æ¢ç´¢å¯†åº¦8å€: 1-40ã‚’40å€‹ã®ãƒã‚¤ãƒ³ãƒˆã§æ¢ç´¢
            min_samples_range = list(range(1, 11)) + list(range(12, 21, 2)) + list(range(22, 41, 3))
        
        best_classix = None
        best_labels = None
        best_score = -1
        best_params = None
        
        for radius in radius_range:
            for min_samples in min_samples_range:
                try:
                    # ãƒ‡ãƒ¼ã‚¿å‹ã‚’float64ã«æ˜ç¤ºçš„ã«å¤‰æ›ï¼ˆdtype mismatchå›é¿ï¼‰
                    X_scaled_float64 = self.X_scaled.astype(np.float64)
                    
                    classix = CLASSIX(
                        radius=radius,
                        minPts=min_samples,
                        verbose=0,
                        post_alloc=True  # ãƒã‚¤ã‚ºãƒã‚¤ãƒ³ãƒˆã‚’è¿‘éš£ã‚¯ãƒ©ã‚¹ã‚¿ã«å†é…ç½®
                    )
                    
                    classix.fit(X_scaled_float64)
                    labels = classix.labels_
                    
                    # ãƒ©ãƒ™ãƒ«ã‚’int64ã«å¤‰æ›ï¼ˆdtypeäº’æ›æ€§ç¢ºä¿ï¼‰
                    labels = np.array(labels, dtype=np.int64)
                    
                    # ã‚¯ãƒ©ã‚¹ã‚¿æ•°è¨ˆç®—ï¼ˆ-1ã¯ãƒã‚¤ã‚ºï¼‰
                    n_clusters = len(set(labels)) - (1 if -1 in labels else 0)
                    n_noise = list(labels).count(-1)
                    
                    # ã‚¯ãƒ©ã‚¹ã‚¿ãŒ2ã¤ä»¥ä¸Šã‚ã‚‹å ´åˆã®ã¿è©•ä¾¡
                    if n_clusters >= 2:
                        # ãƒã‚¤ã‚ºãƒã‚¤ãƒ³ãƒˆã‚’é™¤å¤–ã—ã¦ã‚·ãƒ«ã‚¨ãƒƒãƒˆã‚¹ã‚³ã‚¢ã‚’è¨ˆç®—
                        mask = labels != -1
                        if mask.sum() > 1 and len(set(labels[mask])) > 1:
                            score = silhouette_score(self.X_scaled[mask], labels[mask])
                            
                            # ç›®æ¨™ã‚¯ãƒ©ã‚¹ã‚¿æ•°ã‹ã‚‰ã®è·é›¢ã‚’è¨ˆç®—(ãƒšãƒŠãƒ«ãƒ†ã‚£)
                            cluster_penalty = abs(n_clusters - target_clusters) / target_clusters
                            # ãƒã‚¤ã‚ºæ¯”ç‡ã‚‚è€ƒæ…®
                            noise_penalty = n_noise / len(labels)
                            adjusted_score = score * (1 - cluster_penalty * 0.5) * (1 - noise_penalty * 0.3)
                            
                            print(f"   radius={radius}, minPts={min_samples}: "
                                  f"ã‚¯ãƒ©ã‚¹ã‚¿æ•°={n_clusters}, ãƒã‚¤ã‚º={n_noise}, "
                                  f"ã‚¹ã‚³ã‚¢={score:.4f}, èª¿æ•´å¾Œ={adjusted_score:.4f}")
                            
                            # ãƒã‚¤ã‚ºãŒå°‘ãªãã€èª¿æ•´å¾Œã‚¹ã‚³ã‚¢ãŒé«˜ãã€ã‚¯ãƒ©ã‚¹ã‚¿æ•°ãŒé©åˆ‡ãªã‚‚ã®ã‚’é¸æŠ
                            # v0.7æ”¹: ã‚¯ãƒ©ã‚¹ã‚¿æ•°ç¯„å›²ã‚’50ã€œ150å€‹ã«æ‹¡å¤§ï¼ˆç›®æ¨™112ï¼‰
                            # ãƒã‚¤ã‚ºæ¯”ç‡: 10%ä»¥ä¸‹ï¼ˆå³æ ¼åŒ–ï¼‰
                            if (adjusted_score > best_score and 
                                n_noise < len(labels) * 0.10 and 
                                50 <= n_clusters <= 150):
                                best_score = adjusted_score
                                best_classix = classix
                                best_labels = labels
                                best_params = {'radius': radius, 'minPts': min_samples}
                
                except Exception as e:
                    print(f"   âš ï¸ radius={radius}, minPts={min_samples}ã§ã‚¨ãƒ©ãƒ¼: {e}")
                    continue
        
        if best_labels is not None:
            n_clusters_final = len(set(best_labels)) - (1 if -1 in best_labels else 0)
            n_noise_final = list(best_labels).count(-1)
            
            print(f"   âœ“ æœ€é©ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿: radius={best_params['radius']}, "
                  f"minPts={best_params['minPts']} (èª¿æ•´å¾Œã‚¹ã‚³ã‚¢: {best_score:.4f})")
            print(f"   âœ“ ã‚¯ãƒ©ã‚¹ã‚¿æ•°: {n_clusters_final}, ãƒã‚¤ã‚º: {n_noise_final}ä»¶ ({n_noise_final/len(best_labels)*100:.1f}%)")
            
            # ã‚¯ãƒ©ã‚¹ã‚¿åˆ†å¸ƒã‚’è¡¨ç¤ºï¼ˆä¸Šä½10ä»¶ã®ã¿ï¼‰
            unique, counts = np.unique(best_labels, return_counts=True)
            sorted_clusters = sorted(zip(unique, counts), key=lambda x: x[1], reverse=True)
            print(f"   ä¸»è¦ã‚¯ãƒ©ã‚¹ã‚¿åˆ†å¸ƒï¼ˆä¸Šä½10ä»¶ï¼‰:")
            for i, (cluster_id, count) in enumerate(sorted_clusters[:10]):
                label_name = "ãƒã‚¤ã‚º" if cluster_id == -1 else f"ã‚¯ãƒ©ã‚¹ã‚¿ {cluster_id}"
                print(f"     {label_name}: {count}ä»¶")
            
            self.results['CLASSIX'] = {
                'model': best_classix,
                'labels': best_labels,
                'n_clusters': n_clusters_final,
                'score': best_score,
                'params': best_params
            }
        else:
            print(f"   âš ï¸ é©åˆ‡ãªCLASSIXãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")
            self.results['CLASSIX'] = None
        
        return best_labels
    
    def _try_classix_bayesian(self, target_clusters, n_calls):
        """ãƒ™ã‚¤ã‚ºæœ€é©åŒ–ã‚’ä½¿ç”¨ã—ãŸCLASSIXãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ¢ç´¢
        
        Note: target_clustersã¯äº’æ›æ€§ã®ãŸã‚ã«æ®‹ã—ã¦ã„ã‚‹ãŒã€æœ€é©åŒ–ã§ã¯ä½¿ç”¨ã—ãªã„
        """
        from classix import CLASSIX
        from skopt import forest_minimize  # ä¸¦åˆ—åŒ–ã«é©ã—ãŸæ‰‹æ³•
        from skopt.space import Real, Integer
        from skopt.utils import use_named_args
        
        # æ¢ç´¢ç©ºé–“ã®å®šç¾©ï¼ˆv0.8æ”¹: ã‚ˆã‚Šé©åˆ‡ãªç¯„å›²ã«çµã‚‹ï¼‰
        space = [
            Real(0.3, 0.5, name='radius'),  # ã‚¯ãƒ©ã‚¹ã‚¿æ•°ã‚’å¢—ã‚„ã™ãŸã‚ç¯„å›²ã‚’ç‹­ã‚ã‚‹
            Integer(3, 40, name='minPts')  # ä¸‹é™ã‚’3ã«å¼•ãä¸Šã’
        ]
        
        # ç›®çš„é–¢æ•°ï¼ˆæœ€å°åŒ–ã™ã‚‹ãŸã‚ã€èª¿æ•´å¾Œã‚¹ã‚³ã‚¢ã®è² å€¤ã‚’è¿”ã™ï¼‰
        @use_named_args(space)
        def objective(radius, minPts):
            try:
                # ãƒ‡ãƒ¼ã‚¿å‹ã‚’float64ã«æ˜ç¤ºçš„ã«å¤‰æ›ï¼ˆdtype mismatchå›é¿ï¼‰
                X_scaled_float64 = self.X_scaled.astype(np.float64)
                
                classix = CLASSIX(
                    radius=radius,
                    minPts=minPts,
                    verbose=0,
                    post_alloc=True
                )
                
                classix.fit(X_scaled_float64)
                labels = classix.labels_
                labels = np.array(labels, dtype=np.int64)
                
                # ã‚¯ãƒ©ã‚¹ã‚¿æ•°è¨ˆç®—
                n_clusters = len(set(labels)) - (1 if -1 in labels else 0)
                n_noise = list(labels).count(-1)
                
                # åˆ¶ç´„ãƒã‚§ãƒƒã‚¯
                if n_clusters < 2:
                    return 10.0  # ãƒšãƒŠãƒ«ãƒ†ã‚£å¤§
                
                # ãƒã‚¤ã‚ºãƒã‚¤ãƒ³ãƒˆã‚’é™¤å¤–ã—ã¦ã‚·ãƒ«ã‚¨ãƒƒãƒˆã‚¹ã‚³ã‚¢ã‚’è¨ˆç®—
                mask = labels != -1
                if mask.sum() <= 1 or len(set(labels[mask])) <= 1:
                    return 10.0
                
                # v0.8æ”¹: Silhouetteä¿‚æ•°ã¨Davis-Bouldin Indexã‚’0.5:0.5ã§è©•ä¾¡
                from sklearn.metrics import davies_bouldin_score
                
                silhouette = silhouette_score(self.X_scaled[mask], labels[mask])
                db_index = davies_bouldin_score(self.X_scaled[mask], labels[mask])
                
                # Silhouette: é«˜ã„æ–¹ãŒè‰¯ã„ (0-1), DB Index: ä½ã„æ–¹ãŒè‰¯ã„ (0-)
                # DB Indexã‚’0-1ã«æ­£è¦åŒ–ï¼ˆå…¸å‹çš„ãªç¯„å›²0-3ã‚’æƒ³å®šï¼‰
                db_normalized = max(0, 1.0 - db_index / 3.0)
                
                # ã‚¯ãƒ©ã‚¹ã‚¿ãƒãƒ©ãƒ³ã‚¹ã‚¹ã‚³ã‚¢ï¼ˆå„ã‚¯ãƒ©ã‚¹ã‚¿ã®å‡ç­‰æ€§ã‚’è©•ä¾¡ï¼‰
                unique_labels, counts = np.unique(labels[mask], return_counts=True)
                if len(counts) > 1:
                    # æ¨™æº–åå·®ã‚’ä½¿ç”¨ã—ã¦ä¸å‡è¡¡åº¦ã‚’è¨ˆç®—ï¼ˆ0-1ã«æ­£è¦åŒ–ï¼‰
                    mean_count = counts.mean()
                    std_count = counts.std()
                    balance_score = max(0, 1.0 - std_count / (mean_count + 1e-6))
                else:
                    balance_score = 0.0
                
                # è¤‡åˆã‚¹ã‚³ã‚¢ (0.35:0.35:0.3 = Silhouette:DB:Balance)
                # v0.8æ”¹: ãƒãƒ©ãƒ³ã‚¹ã‚¹ã‚³ã‚¢ã®é‡ã¿ã‚’0.2â†’0.3ã«å¢—åŠ 
                combined_score = 0.35 * silhouette + 0.35 * db_normalized + 0.3 * balance_score
                
                noise_ratio = n_noise / len(labels)
                
                # ã‚¯ãƒ©ã‚¹ã‚¿æ•°ãƒšãƒŠãƒ«ãƒ†ã‚£ï¼ˆ10ä»¥ä¸Šã€76ä»¥ä¸‹ã‚’å¼·åˆ¶ã€50-76ãŒç†æƒ³ç¯„å›²ï¼‰
                # 76 = è‡ªæ²»ä½“ç·æ•°19 Ã— 4
                if n_clusters < 10:
                    combined_score *= 0.1  # 90%æ¸›ç‚¹ï¼ˆéå¸¸ã«å¤§ããªãƒšãƒŠãƒ«ãƒ†ã‚£ï¼‰
                elif n_clusters < 50:
                    combined_score *= 0.7  # 30%æ¸›ç‚¹
                elif n_clusters > 76:
                    combined_score *= 0.1  # 90%æ¸›ç‚¹ï¼ˆä¸Šé™è¶…éã‚‚å³ã—ããƒšãƒŠãƒ«ãƒ†ã‚£ï¼‰
                
                # ãƒã‚¤ã‚ºãƒšãƒŠãƒ«ãƒ†ã‚£ï¼ˆ10%æœªæº€ã¯è»½å¾®ã€10%ä»¥ä¸Šã¯å¤§ããªãƒšãƒŠãƒ«ãƒ†ã‚£ï¼‰
                if noise_ratio < 0.10:
                    combined_score *= (1.0 - noise_ratio * 0.5)  # æœ€å¤§5%æ¸›ç‚¹
                else:
                    combined_score *= max(0.01, 1.0 - noise_ratio * 2.0)  # å¤§å¹…æ¸›ç‚¹
                
                return -combined_score  # æœ€å°åŒ–ã™ã‚‹ãŸã‚è² å€¤
                
            except Exception as e:
                return 10.0  # ã‚¨ãƒ©ãƒ¼æ™‚ã¯å¤§ããªãƒšãƒŠãƒ«ãƒ†ã‚£
        
        # ãƒ™ã‚¤ã‚ºæœ€é©åŒ–ã®å®Ÿè¡Œï¼ˆé€²æ—è¡¨ç¤ºä»˜ãï¼‰
        print(f"   ãƒ™ã‚¤ã‚ºæœ€é©åŒ–ã‚’é–‹å§‹ï¼ˆ{n_calls}å›ã®è©•ä¾¡ï¼‰...")
        
        # ã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯é–¢æ•°ã§é€²æ—è¡¨ç¤º
        iteration = [0]
        def on_step(res):
            iteration[0] += 1
            if iteration[0] % 10 == 0 or iteration[0] <= 5:
                best_score = -res.fun
                print(f"      è©•ä¾¡ {iteration[0]}/{n_calls}: ç¾åœ¨ã®æœ€è‰¯ã‚¹ã‚³ã‚¢ = {best_score:.4f}")
        
        # ãƒ™ã‚¤ã‚ºæœ€é©åŒ–å®Ÿè¡Œï¼ˆforest_minimizeã§ä¸¦åˆ—å‡¦ç†æœ‰åŠ¹ï¼‰
        cores_msg = "å…¨CPUã‚³ã‚¢" if config.N_JOBS == -1 else f"{config.N_JOBS}ã‚³ã‚¢"
        print(f"   ğŸš€ Random Forestãƒ™ãƒ¼ã‚¹ã§{cores_msg}ä¸¦åˆ—å‡¦ç†ã‚’å®Ÿè¡Œ...")
        print(f"   ğŸ“Š åˆæœŸãƒ©ãƒ³ãƒ€ãƒ æ¢ç´¢: {config.N_INITIAL_POINTS}å›(ä¸¦åˆ—)ã€é€æ¬¡æ¢ç´¢: {n_calls - config.N_INITIAL_POINTS}å›")
        result = forest_minimize(
            objective,
            space,
            n_calls=n_calls,
            random_state=config.RANDOM_STATE,
            verbose=False,
            n_initial_points=config.N_INITIAL_POINTS,
            n_jobs=config.N_JOBS,  # ä¸¦åˆ—å‡¦ç†
            callback=[on_step]
        )
        
        # æœ€é©ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã§æœ€çµ‚ãƒ¢ãƒ‡ãƒ«ã‚’æ§‹ç¯‰
        best_radius = result.x[0]
        best_minPts = result.x[1]
        best_score = -result.fun  # è² å€¤ã‚’æˆ»ã™
        
        print(f"   âœ“ æœ€é©ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿: radius={best_radius:.3f}, minPts={best_minPts}")
        print(f"   âœ“ èª¿æ•´å¾Œã‚¹ã‚³ã‚¢: {best_score:.4f}")
        
        # æœ€é©ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã§æœ€çµ‚ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°
        X_scaled_float64 = self.X_scaled.astype(np.float64)
        best_classix = CLASSIX(
            radius=best_radius,
            minPts=best_minPts,
            verbose=0,
            post_alloc=True
        )
        best_classix.fit(X_scaled_float64)
        best_labels = np.array(best_classix.labels_, dtype=np.int64)
        
        n_clusters_final = len(set(best_labels)) - (1 if -1 in best_labels else 0)
        n_noise_final = list(best_labels).count(-1)
        
        print(f"   âœ“ ã‚¯ãƒ©ã‚¹ã‚¿æ•°: {n_clusters_final}, ãƒã‚¤ã‚º: {n_noise_final}ä»¶ ({n_noise_final/len(best_labels)*100:.1f}%)")
        
        # ã‚¯ãƒ©ã‚¹ã‚¿åˆ†å¸ƒã‚’è¡¨ç¤ºï¼ˆä¸Šä½10ä»¶ã®ã¿ï¼‰
        unique, counts = np.unique(best_labels, return_counts=True)
        sorted_clusters = sorted(zip(unique, counts), key=lambda x: x[1], reverse=True)
        print(f"   ä¸»è¦ã‚¯ãƒ©ã‚¹ã‚¿åˆ†å¸ƒï¼ˆä¸Šä½10ä»¶ï¼‰:")
        for i, (cluster_id, count) in enumerate(sorted_clusters[:10]):
            label_name = "ãƒã‚¤ã‚º" if cluster_id == -1 else f"ã‚¯ãƒ©ã‚¹ã‚¿ {cluster_id}"
            print(f"     {label_name}: {count}ä»¶")
        
        self.results['CLASSIX'] = {
            'model': best_classix,
            'labels': best_labels,
            'n_clusters': n_clusters_final,
            'score': best_score,
            'params': {'radius': best_radius, 'minPts': best_minPts}
        }
        
        return best_labels
    
    def get_results(self):
        """ã™ã¹ã¦ã®çµæœã‚’è¿”ã™"""
        return self.results


class AlternativeDimensionalityReduction:
    """ä»£æ›¿æ¬¡å…ƒå‰Šæ¸›æ‰‹æ³•ã‚’æä¾›ã™ã‚‹ã‚¯ãƒ©ã‚¹"""
    
    def __init__(self, X_scaled):
        """
        Parameters:
        -----------
        X_scaled : array-like
            æ¨™æº–åŒ–ã•ã‚ŒãŸç‰¹å¾´é‡
        """
        self.X_scaled = X_scaled
        self.results = {}
    
    def try_tsne(self, n_components=2, perplexity_range=None):
        """t-SNEï¼ˆt-distributed Stochastic Neighbor Embeddingï¼‰"""
        print(f"\nğŸ”´ t-SNE ã‚’å®Ÿè¡Œä¸­...")
        
        from sklearn.manifold import TSNE
        
        if perplexity_range is None:
            # ãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚ºã«å¿œã˜ã¦é©åˆ‡ãªperplexityã‚’é¸æŠ
            n_samples = len(self.X_scaled)
            perplexity_range = [10,
                               min(30, n_samples // 4), 
                               min(50, n_samples // 3)]
        
        best_tsne = None
        best_embedding = None
        best_perplexity = perplexity_range[0]
        
        for perplexity in perplexity_range:
            try:
                # scikit-learnã®ãƒãƒ¼ã‚¸ãƒ§ãƒ³ã«å¿œã˜ã¦ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’èª¿æ•´
                tsne_params = {
                    'n_components': n_components,
                    'perplexity': perplexity,
                    'random_state': config.RANDOM_STATE
                }
                
                # ãƒãƒ¼ã‚¸ãƒ§ãƒ³äº’æ›æ€§ã®ãŸã‚ã€max_iterã¨n_iterã®ä¸¡æ–¹ã‚’è©¦ã™
                try:
                    tsne = TSNE(**tsne_params, n_iter=1000, n_iter_without_progress=300)
                except TypeError:
                    tsne = TSNE(**tsne_params, max_iter=1000, n_iter_without_progress=300)
                
                embedding = tsne.fit_transform(self.X_scaled)
                
                # KL divergenceãŒåˆ©ç”¨å¯èƒ½ãªå ´åˆã¯è¡¨ç¤º
                if hasattr(tsne, 'kl_divergence_'):
                    print(f"   perplexity={perplexity}: KL divergence = {tsne.kl_divergence_:.4f}")
                    if best_tsne is None or tsne.kl_divergence_ < best_tsne.kl_divergence_:
                        best_tsne = tsne
                        best_embedding = embedding
                        best_perplexity = perplexity
                else:
                    print(f"   perplexity={perplexity}: å®Œäº†")
                    if best_tsne is None:
                        best_tsne = tsne
                        best_embedding = embedding
                        best_perplexity = perplexity
            
            except Exception as e:
                print(f"   âš ï¸ perplexity={perplexity}ã§ã‚¨ãƒ©ãƒ¼: {e}")
                continue
        
        if best_embedding is not None:
            print(f"   âœ“ æœ€é©perplexity: {best_perplexity}")
            
            self.results['t-SNE'] = {
                'model': best_tsne,
                'embedding': best_embedding,
                'perplexity': best_perplexity
            }
        else:
            print(f"   âš ï¸ t-SNEã®å®Ÿè¡Œã«å¤±æ•—ã—ã¾ã—ãŸã€‚")
            self.results['t-SNE'] = None
        
        return best_embedding
    
    def try_umap(self, n_components=2, n_neighbors_range=None, create_3d=False):
        """UMAPï¼ˆUniform Manifold Approximation and Projectionï¼‰
        
        Parameters:
        -----------
        n_components : int
            æ¬¡å…ƒæ•°ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 2ï¼‰
        n_neighbors_range : list, optional
            æ¢ç´¢ã™ã‚‹n_neighborsã®ç¯„å›²
        create_3d : bool
            3æ¬¡å…ƒåŸ‹ã‚è¾¼ã¿ã‚‚ä½œæˆã™ã‚‹ã‹ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: Falseï¼‰
        """
        print(f"\nğŸŸ  UMAP ã‚’å®Ÿè¡Œä¸­...")
        
        try:
            import umap
        except ImportError:
            print(f"   âš ï¸ UMAPãŒã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚")
            print(f"   'pip install umap-learn' ã§ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã—ã¦ãã ã•ã„ã€‚")
            self.results['UMAP'] = None
            return None
        
        if n_neighbors_range is None:
            n_samples = len(self.X_scaled)
            n_neighbors_range = [min(15, n_samples // 10),
                                 min(30, n_samples // 5)]
        
        best_umap = None
        best_embedding = None
        best_n_neighbors = n_neighbors_range[0]
        
        for n_neighbors in n_neighbors_range:
            try:
                umap_model = umap.UMAP(
                    n_components=n_components,
                    n_neighbors=n_neighbors,
                    random_state=config.RANDOM_STATE,
                    min_dist=0.1
                )
                
                embedding = umap_model.fit_transform(self.X_scaled)
                
                print(f"   n_neighbors={n_neighbors}: å®Œäº†")
                
                if best_umap is None:
                    best_umap = umap_model
                    best_embedding = embedding
                    best_n_neighbors = n_neighbors
            
            except Exception as e:
                print(f"   âš ï¸ n_neighbors={n_neighbors}ã§ã‚¨ãƒ©ãƒ¼: {e}")
                continue
        
        # 3æ¬¡å…ƒåŸ‹ã‚è¾¼ã¿ã®ä½œæˆ
        embedding_3d = None
        if best_embedding is not None and create_3d:
            print(f"\nğŸŸ  UMAP 3æ¬¡å…ƒã‚’å®Ÿè¡Œä¸­...")
            try:
                umap_model_3d = umap.UMAP(
                    n_components=3,
                    n_neighbors=best_n_neighbors,
                    random_state=config.RANDOM_STATE,
                    min_dist=0.1
                )
                embedding_3d = umap_model_3d.fit_transform(self.X_scaled)
                print(f"   âœ“ 3æ¬¡å…ƒåŸ‹ã‚è¾¼ã¿ä½œæˆå®Œäº†")
            except Exception as e:
                print(f"   âš ï¸ 3æ¬¡å…ƒåŸ‹ã‚è¾¼ã¿ã§ã‚¨ãƒ©ãƒ¼: {e}")
        
        if best_embedding is not None:
            print(f"   âœ“ æœ€é©n_neighbors: {best_n_neighbors}")
            
            self.results['UMAP'] = {
                'model': best_umap,
                'embedding': best_embedding,
                'embedding_3d': embedding_3d,
                'n_neighbors': best_n_neighbors
            }
        else:
            print(f"   âš ï¸ UMAPã®å®Ÿè¡Œã«å¤±æ•—ã—ã¾ã—ãŸã€‚")
            self.results['UMAP'] = None
        
        return best_embedding
    
    def get_results(self):
        """ã™ã¹ã¦ã®çµæœã‚’è¿”ã™"""
        return self.results
