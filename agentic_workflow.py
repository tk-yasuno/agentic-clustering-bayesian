# -*- coding: utf-8 -*-
"""
Agenticã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼: v0.8
è‡ªå·±è©•ä¾¡ã¨æ”¹å–„ã‚’ç¹°ã‚Šè¿”ã™è³¢ã„ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ã‚·ã‚¹ãƒ†ãƒ 
"""

import pandas as pd
import numpy as np
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
import config
from cluster_evaluator import ClusterEvaluator, DimensionalityReductionEvaluator, compare_methods
from alternative_methods import AlternativeClusteringMethods, AlternativeDimensionalityReduction

class AgenticClusteringWorkflow:
    """Agenticã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ã®ãƒ¡ã‚¤ãƒ³ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼"""
    
    def __init__(self, df, feature_cols):
        """
        Parameters:
        -----------
        df : DataFrame
            å‰å‡¦ç†æ¸ˆã¿ãƒ‡ãƒ¼ã‚¿
        feature_cols : list
            ä½¿ç”¨ã™ã‚‹ç‰¹å¾´é‡ã®ã‚«ãƒ©ãƒ åãƒªã‚¹ãƒˆ
        """
        self.df = df
        self.feature_cols = feature_cols
        self.X = df[feature_cols].fillna(0)
        self.X_scaled = None
        self.scaler = None
        
        # çµæœã‚’ä¿å­˜
        self.clustering_results = {}
        self.evaluation_results = {}
        self.dimensionality_results = {}
        self.dim_evaluation_results = {}
        
        # æœ€çµ‚çš„ãªé¸æŠ
        self.best_clustering_method = None
        self.best_clustering_labels = None
        self.best_dim_reduction_method = None
        self.best_dim_reduction_embedding = None
        
        # æ”¹å–„å±¥æ­´
        self.improvement_log = []
    
    def run(self, quality_threshold=60.0, overlap_threshold=0.5):
        """
        Agenticãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ã‚’å®Ÿè¡Œ
        
        Parameters:
        -----------
        quality_threshold : float
            ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°å“è³ªã®é–¾å€¤ï¼ˆ0-100ï¼‰
        overlap_threshold : float
            æ¬¡å…ƒå‰Šæ¸›ã®ã‚ªãƒ¼ãƒãƒ¼ãƒ©ãƒƒãƒ—é–¾å€¤ï¼ˆ0-1ï¼‰
        """
        print("\n" + "="*70)
        print("ğŸ¤– Agenticã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ v0.8")
        print("="*70)
        
        # ã‚¹ãƒ†ãƒƒãƒ—1: ãƒ‡ãƒ¼ã‚¿æ¨™æº–åŒ–
        self._standardize_data()
        
        # ã‚¹ãƒ†ãƒƒãƒ—2: åˆå›ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ï¼ˆKMeansï¼‰
        self._log("ã€ãƒ©ã‚¦ãƒ³ãƒ‰1ã€‘åˆå›ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ï¼ˆKMeansï¼‰")
        initial_labels = self._initial_clustering()
        
        # ã‚¹ãƒ†ãƒƒãƒ—3: ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°å“è³ªè©•ä¾¡
        self._log("ã€è©•ä¾¡1ã€‘ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°å“è³ªè©•ä¾¡")
        evaluator = ClusterEvaluator(self.X_scaled, initial_labels)
        initial_scores = evaluator.evaluate_all()
        self.evaluation_results['KMeans'] = initial_scores
        
        # ã‚¹ãƒ†ãƒƒãƒ—4: æ”¹å–„ãŒå¿…è¦ã‹åˆ¤å®š
        needs_improvement = evaluator.needs_improvement(quality_threshold)
        
        if needs_improvement:
            # ã‚¹ãƒ†ãƒƒãƒ—5: ä»£æ›¿ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°æ‰‹æ³•ã®å®Ÿè¡Œ
            self._log("ã€ãƒ©ã‚¦ãƒ³ãƒ‰2ã€‘ä»£æ›¿ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°æ‰‹æ³•ã®è©¦è¡Œ")
            self._try_alternative_clustering()
            
            # ã‚¹ãƒ†ãƒƒãƒ—6: æœ€é©æ‰‹æ³•ã®é¸æŠ
            self._log("ã€é¸æŠã€‘æœ€é©ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°æ‰‹æ³•ã®æ±ºå®š")
            self._select_best_clustering()
        else:
            self.best_clustering_method = 'KMeans'
            self.best_clustering_labels = initial_labels
            self._log("âœ… åˆå›ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ã§ååˆ†ãªå“è³ªãŒå¾—ã‚‰ã‚Œã¾ã—ãŸ")
        
        # ã‚¹ãƒ†ãƒƒãƒ—7: æ¬¡å…ƒå‰Šæ¸›ï¼ˆv0.8æ”¹: PCAã¯å›£å­çŠ¶æ…‹ã«ãªã‚‹ãŸã‚ã€t-SNE/UMAPã‚’ç›´æ¥ä½¿ç”¨ï¼‰
        self._log("ã€ãƒ©ã‚¦ãƒ³ãƒ‰1ã€‘æ¬¡å…ƒå‰Šæ¸›ï¼ˆt-SNE/UMAPï¼‰")
        
        # ã‚¹ãƒ†ãƒƒãƒ—8: ä»£æ›¿æ¬¡å…ƒå‰Šæ¸›æ‰‹æ³•ã®å®Ÿè¡Œï¼ˆPCAã‚’ã‚¹ã‚­ãƒƒãƒ—ï¼‰
        self._log("ã€å®Ÿè¡Œã€‘t-SNE/UMAPæ¬¡å…ƒå‰Šæ¸›")
        self._try_alternative_dimensionality_reduction()
        
        # ã‚¹ãƒ†ãƒƒãƒ—9: æœ€é©æ¬¡å…ƒå‰Šæ¸›æ‰‹æ³•ã®é¸æŠ
        self._log("ã€é¸æŠã€‘æœ€é©æ¬¡å…ƒå‰Šæ¸›æ‰‹æ³•ã®æ±ºå®š")
        self._select_best_dimensionality_reduction()
        
        # ã‚¹ãƒ†ãƒƒãƒ—12: çµæœã®ã‚µãƒãƒªãƒ¼
        self._print_summary()
        
        return self._prepare_final_results()
    
    def _standardize_data(self):
        """ãƒ‡ãƒ¼ã‚¿ã‚’æ¨™æº–åŒ–"""
        print("\nğŸ“Š ç‰¹å¾´é‡ã‚’æ¨™æº–åŒ–ä¸­...")
        self.scaler = StandardScaler()
        self.X_scaled = self.scaler.fit_transform(self.X)
        print("âœ“ æ¨™æº–åŒ–å®Œäº†")
    
    def _initial_clustering(self):
        """åˆå›ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ï¼ˆKMeansï¼‰"""
        from clustering import find_optimal_clusters, perform_clustering
        
        best_k, best_score, _ = find_optimal_clusters(self.X_scaled)
        kmeans, labels = perform_clustering(self.X_scaled, best_k)
        
        self.clustering_results['KMeans'] = {
            'model': kmeans,
            'labels': labels,
            'n_clusters': best_k
        }
        
        return labels
    
    def _try_alternative_clustering(self):
        """ä»£æ›¿ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°æ‰‹æ³•ã‚’è©¦è¡Œ"""
        alt_methods = AlternativeClusteringMethods(self.X_scaled)
        
        # åˆå›ã®ã‚¯ãƒ©ã‚¹ã‚¿æ•°ã‚’å‚è€ƒå€¤ã¨ã—ã¦ä½¿ç”¨
        reference_k = self.clustering_results['KMeans']['n_clusters']
        
        # GMMï¼ˆGaussian Mixture Modelï¼‰
        # v0.8æ”¹: ãƒ™ã‚¤ã‚ºæœ€é©åŒ–ã‚’ä½¿ç”¨ã—ã¦ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ¢ç´¢
        try:
            gmm_labels = alt_methods.try_gmm(
                use_bayesian=True,
                n_calls=100
            )
            self.clustering_results['GMM'] = alt_methods.results['GMM']
            
            # è©•ä¾¡
            evaluator = ClusterEvaluator(self.X_scaled, gmm_labels)
            self.evaluation_results['GMM'] = evaluator.evaluate_all()
            print(f"   âœ… GMMã‚’ä»£æ›¿æ‰‹æ³•å€™è£œã«è¿½åŠ ã—ã¾ã—ãŸã€‚")
        except Exception as e:
            print(f"   âš ï¸ GMMå®Ÿè¡Œã‚¨ãƒ©ãƒ¼: {e}")
        
        # DBSCAN
        # v0.8æ”¹: ãƒ™ã‚¤ã‚ºæœ€é©åŒ–ã‚’ä½¿ç”¨ã—ã¦ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ¢ç´¢ã‚’åŠ¹ç‡åŒ–
        try:
            dbscan_labels = alt_methods.try_dbscan(
                use_bayesian=True,
                n_calls=100
            )
            self.clustering_results['DBSCAN'] = alt_methods.results['DBSCAN']
            dbscan_n_clusters = alt_methods.results['DBSCAN']['n_clusters']
            
            # è©•ä¾¡ï¼ˆãƒã‚¤ã‚ºãƒã‚¤ãƒ³ãƒˆã‚’é™¤å¤–ï¼‰
            mask = dbscan_labels != -1
            if mask.sum() > 1:
                evaluator = ClusterEvaluator(self.X_scaled[mask], dbscan_labels[mask])
                self.evaluation_results['DBSCAN'] = evaluator.evaluate_all()
                print(f"   âœ… DBSCANã‚’ä»£æ›¿æ‰‹æ³•å€™è£œã«è¿½åŠ ã—ã¾ã—ãŸã€‚")
                    
        except Exception as e:
            print(f"   âš ï¸ DBSCANå®Ÿè¡Œã‚¨ãƒ©ãƒ¼: {e}")
        
        # ğŸ†• v0.7: HDBSCANã¨CLASSIXã‚’å¸¸ã«è©¦è¡Œï¼ˆDBSCANã®çµæœã«é–¢ä¿‚ãªãï¼‰
        print(f"\nğŸ”„ å¯†åº¦ãƒ™ãƒ¼ã‚¹é«˜åº¦æ‰‹æ³•ã‚’è©¦è¡Œã—ã¾ã™...")
        print(f"   â†’ HDBSCANç›®æ¨™ã‚¯ãƒ©ã‚¹ã‚¿æ•°: {config.HDBSCAN_TARGET_CLUSTERS}")
        print(f"   â†’ CLASSIXç›®æ¨™ã‚¯ãƒ©ã‚¹ã‚¿æ•°: {config.HDBSCAN_TARGET_CLUSTERS}")
        
        # HDBSCAN
        # v0.8æ”¹: ãƒ™ã‚¤ã‚ºæœ€é©åŒ–ã‚’ä½¿ç”¨ã—ã¦ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ¢ç´¢ã‚’åŠ¹ç‡åŒ–
        try:
            hdbscan_labels = alt_methods.try_hdbscan(
                target_clusters=config.HDBSCAN_TARGET_CLUSTERS,
                use_bayesian=True,
                n_calls=100
            )
            if hdbscan_labels is not None and 'HDBSCAN' in alt_methods.results:
                self.clustering_results['HDBSCAN'] = alt_methods.results['HDBSCAN']
                
                # è©•ä¾¡ï¼ˆãƒã‚¤ã‚ºãƒã‚¤ãƒ³ãƒˆã‚’é™¤å¤–ï¼‰
                mask_hdbscan = hdbscan_labels != -1
                if mask_hdbscan.sum() > 1:
                    evaluator_hdbscan = ClusterEvaluator(self.X_scaled[mask_hdbscan], hdbscan_labels[mask_hdbscan])
                    self.evaluation_results['HDBSCAN'] = evaluator_hdbscan.evaluate_all()
                    print(f"   âœ… HDBSCANã‚’ä»£æ›¿æ‰‹æ³•å€™è£œã«è¿½åŠ ã—ã¾ã—ãŸã€‚")
        except Exception as e:
            print(f"   âš ï¸ HDBSCANå®Ÿè¡Œã‚¨ãƒ©ãƒ¼: {e}")
        
        # CLASSIXï¼ˆHDBSCANã®ãƒã‚¤ã‚ºå•é¡Œã‚’è§£æ±ºã™ã‚‹ä»£æ›¿æ‰‹æ³•ï¼‰
        # âœ… Windows Cython dtypeå•é¡Œã‚’è§£æ±ºï¼ˆmerge_cm_win.pyxã‚’ä¿®æ­£ã—ã¦np.int64ã«å¤‰æ›´ï¼‰
        # v0.8æ”¹: ãƒ™ã‚¤ã‚ºæœ€é©åŒ–ã‚’ä½¿ç”¨ã—ã¦ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ¢ç´¢ã‚’åŠ¹ç‡åŒ–
        try:
            classix_labels = alt_methods.try_classix(
                target_clusters=config.CLASSIX_TARGET_CLUSTERS,
                use_bayesian=True,
                n_calls=100
            )
            if classix_labels is not None and 'CLASSIX' in alt_methods.results:
                self.clustering_results['CLASSIX'] = alt_methods.results['CLASSIX']
                
                # è©•ä¾¡ï¼ˆãƒã‚¤ã‚ºãƒã‚¤ãƒ³ãƒˆã‚’é™¤å¤–ï¼‰
                mask_classix = classix_labels != -1
                if mask_classix.sum() > 1:
                    evaluator_classix = ClusterEvaluator(self.X_scaled[mask_classix], classix_labels[mask_classix])
                    self.evaluation_results['CLASSIX'] = evaluator_classix.evaluate_all()
                    print(f"   âœ… CLASSIXã‚’ä»£æ›¿æ‰‹æ³•å€™è£œã«è¿½åŠ ã—ã¾ã—ãŸã€‚")
        except Exception as e:
            print(f"   âš ï¸ CLASSIXå®Ÿè¡Œã‚¨ãƒ©ãƒ¼: {e}")
    
    def _select_best_clustering(self):
        """æœ€é©ãªã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°æ‰‹æ³•ã‚’é¸æŠ"""
        # é™¤å¤–åŸºæº–: 1) DBSCANã®ã‚¯ãƒ©ã‚¹ã‚¿æ•°éå¤šã€2) HDBSCANã®ãƒã‚¤ã‚º10%ä»¥ä¸Š
        filtered_results = {}
        for method, result in self.evaluation_results.items():
            if method == 'DBSCAN':
                n_clusters = self.clustering_results[method].get('n_clusters', 0)
                if n_clusters > config.DBSCAN_CLUSTER_THRESHOLD:
                    print(f"\nâš ï¸  DBSCANã®ã‚¯ãƒ©ã‚¹ã‚¿æ•°({n_clusters})ãŒé–¾å€¤({config.DBSCAN_CLUSTER_THRESHOLD})ã‚’è¶…ãˆã¦ã„ã‚‹ãŸã‚ã€æ¡ç”¨å€™è£œã‹ã‚‰é™¤å¤–ã—ã¾ã™ã€‚")
                    continue
            elif method == 'HDBSCAN':
                # v0.7æ”¹: ãƒã‚¤ã‚º10%ä»¥ä¸Šã®HDBSCANã‚’é™¤å¤–
                labels = self.clustering_results[method]['labels']
                n_noise = list(labels).count(-1)
                noise_ratio = n_noise / len(labels)
                if noise_ratio >= config.HDBSCAN_NOISE_THRESHOLD:
                    print(f"\nâš ï¸  HDBSCANã®ãƒã‚¤ã‚ºæ¯”ç‡({noise_ratio*100:.1f}%)ãŒé–¾å€¤({config.HDBSCAN_NOISE_THRESHOLD*100:.0f}%)ä»¥ä¸Šã®ãŸã‚ã€æ¡ç”¨å€™è£œã‹ã‚‰é™¤å¤–ã—ã¾ã™ã€‚")
                    continue
            filtered_results[method] = result
        
        # ãƒ•ã‚£ãƒ«ã‚¿å¾Œã®å€™è£œã‹ã‚‰æœ€é©æ‰‹æ³•ã‚’é¸æŠ
        if not filtered_results:
            print("\nâš ï¸  ã™ã¹ã¦ã®æ‰‹æ³•ãŒé™¤å¤–ã•ã‚Œã¾ã—ãŸã€‚KMeansã‚’ä½¿ç”¨ã—ã¾ã™ã€‚")
            best_method = 'KMeans'
        else:
            best_method, comparison = compare_methods(filtered_results)
        
        self.best_clustering_method = best_method
        self.best_clustering_labels = self.clustering_results[best_method]['labels']
        
        self._log(f"ğŸ¯ é¸æŠã•ã‚ŒãŸæ‰‹æ³•: {best_method}")
    
    def _initial_dimensionality_reduction(self):
        """åˆå›æ¬¡å…ƒå‰Šæ¸›ï¼ˆPCAï¼‰"""
        pca = PCA(n_components=2)
        embedding = pca.fit_transform(self.X_scaled)
        
        explained_variance = pca.explained_variance_ratio_
        print(f"\nğŸ” PCA: èª¬æ˜ã•ã‚ŒãŸåˆ†æ•£ = {explained_variance.sum():.2%}")
        
        self.dimensionality_results['PCA'] = {
            'model': pca,
            'embedding': embedding
        }
        
        return embedding
    
    def _try_alternative_dimensionality_reduction(self):
        """ä»£æ›¿æ¬¡å…ƒå‰Šæ¸›æ‰‹æ³•ã‚’è©¦è¡Œ"""
        alt_dim = AlternativeDimensionalityReduction(self.X_scaled)
        
        # t-SNE
        try:
            tsne_embedding = alt_dim.try_tsne()
            if tsne_embedding is not None:
                self.dimensionality_results['t-SNE'] = alt_dim.results['t-SNE']
                
                # ã‚ªãƒ¼ãƒãƒ¼ãƒ©ãƒƒãƒ—è©•ä¾¡
                evaluator = DimensionalityReductionEvaluator(
                    tsne_embedding,
                    self.best_clustering_labels
                )
                self.dim_evaluation_results['t-SNE'] = evaluator.evaluate_overlap()
        except Exception as e:
            print(f"   âš ï¸ t-SNEå®Ÿè¡Œã‚¨ãƒ©ãƒ¼: {e}")
        
        # UMAP
        try:
            umap_embedding = alt_dim.try_umap(create_3d=True)  # 3æ¬¡å…ƒã‚‚ä½œæˆ
            if umap_embedding is not None:
                self.dimensionality_results['UMAP'] = alt_dim.results['UMAP']
                
                # ã‚ªãƒ¼ãƒãƒ¼ãƒ©ãƒƒãƒ—è©•ä¾¡
                evaluator = DimensionalityReductionEvaluator(
                    umap_embedding,
                    self.best_clustering_labels
                )
                self.dim_evaluation_results['UMAP'] = evaluator.evaluate_overlap()
        except Exception as e:
            print(f"   âš ï¸ UMAPå®Ÿè¡Œã‚¨ãƒ©ãƒ¼: {e}")
    
    def _select_best_dimensionality_reduction(self):
        """æœ€é©ãªæ¬¡å…ƒå‰Šæ¸›æ‰‹æ³•ã‚’é¸æŠ"""
        print("\n" + "="*70)
        print("ğŸ† æ¬¡å…ƒå‰Šæ¸›æ‰‹æ³•ã®æ¯”è¼ƒ")
        print("="*70)
        
        best_method = 'PCA'
        best_score = self.dim_evaluation_results.get('PCA', {}).get('overlap', float('inf'))
        
        print(f"\næ‰‹æ³• | ã‚ªãƒ¼ãƒãƒ¼ãƒ©ãƒƒãƒ—ã‚¹ã‚³ã‚¢ (ä½â†“)")
        print("-" * 70)
        
        for method, scores in self.dim_evaluation_results.items():
            overlap = scores.get('overlap', float('inf'))
            marker = "ğŸ¥‡" if overlap < best_score else "  "
            print(f"{marker} {method:10s} | {overlap:.4f}")
            
            if overlap < best_score:
                best_score = overlap
                best_method = method
        
        self.best_dim_reduction_method = best_method
        self.best_dim_reduction_embedding = self.dimensionality_results[best_method]['embedding']
        
        print(f"\nğŸ¯ é¸æŠã•ã‚ŒãŸæ‰‹æ³•: {best_method} (ã‚ªãƒ¼ãƒãƒ¼ãƒ©ãƒƒãƒ—: {best_score:.4f})")
        self._log(f"ğŸ¯ æ¬¡å…ƒå‰Šæ¸›æ‰‹æ³•: {best_method}")
    
    def _print_summary(self):
        """çµæœã®ã‚µãƒãƒªãƒ¼ã‚’è¡¨ç¤º"""
        print("\n" + "="*70)
        print("ğŸ“‹ Agenticã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚° æœ€çµ‚çµæœ")
        print("="*70)
        
        print(f"\nğŸ¯ æœ€é©ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°æ‰‹æ³•: {self.best_clustering_method}")
        if self.best_clustering_method in self.evaluation_results:
            scores = self.evaluation_results[self.best_clustering_method]
            print(f"   ç·åˆã‚¹ã‚³ã‚¢: {scores['overall']:.2f}/100")
            print(f"   ã‚·ãƒ«ã‚¨ãƒƒãƒˆã‚¹ã‚³ã‚¢: {scores['silhouette']:.4f}")
        
        print(f"\nğŸ¯ æœ€é©æ¬¡å…ƒå‰Šæ¸›æ‰‹æ³•: {self.best_dim_reduction_method}")
        if self.best_dim_reduction_method in self.dim_evaluation_results:
            scores = self.dim_evaluation_results[self.best_dim_reduction_method]
            print(f"   ã‚ªãƒ¼ãƒãƒ¼ãƒ©ãƒƒãƒ—ã‚¹ã‚³ã‚¢: {scores['overlap']:.4f}")
        
        print(f"\nğŸ“ æ”¹å–„å±¥æ­´:")
        for i, log in enumerate(self.improvement_log, 1):
            print(f"   {i}. {log}")
    
    def _log(self, message):
        """æ”¹å–„ãƒ­ã‚°ã‚’è¨˜éŒ²"""
        print(f"\n{'='*70}")
        print(message)
        print('='*70)
        self.improvement_log.append(message)
    
    def _prepare_final_results(self):
        """æœ€çµ‚çµæœã‚’æº–å‚™"""
        df_result = self.df.copy()
        df_result['cluster'] = self.best_clustering_labels
        
        # ã‚¯ãƒ©ã‚¹ã‚¿ã‚µãƒãƒªãƒ¼
        cluster_summary = df_result.groupby('cluster')[self.feature_cols].mean()
        
        # 3æ¬¡å…ƒåŸ‹ã‚è¾¼ã¿ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ï¼ˆUMAPä½¿ç”¨æ™‚ï¼‰
        embedding_3d = None
        if self.best_dim_reduction_method == 'UMAP' and 'UMAP' in self.dimensionality_results:
            umap_result = self.dimensionality_results['UMAP']
            if 'embedding_3d' in umap_result:
                embedding_3d = umap_result['embedding_3d']
        
        result = {
            'df_with_cluster': df_result,
            'cluster_summary': cluster_summary,
            'embedding': self.best_dim_reduction_embedding,
            'labels': self.best_clustering_labels,
            'clustering_method': self.best_clustering_method,
            'dim_reduction_method': self.best_dim_reduction_method,
            'evaluation_scores': self.evaluation_results.get(self.best_clustering_method, {}),
            'improvement_log': self.improvement_log
        }
        
        # 3æ¬¡å…ƒåŸ‹ã‚è¾¼ã¿ãŒã‚ã‚Œã°è¿½åŠ 
        if embedding_3d is not None:
            result['embedding_3d'] = embedding_3d
        
        return result
