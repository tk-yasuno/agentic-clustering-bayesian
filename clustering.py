# -*- coding: utf-8 -*-
"""
ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ãƒ¡ã‚¤ãƒ³ã‚¹ã‚¯ãƒªãƒ—ãƒˆ: å±±å£çœŒæ©‹æ¢ç¶­æŒç®¡ç†ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°MVP
- KMeansã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°
- ã‚·ãƒ«ã‚¨ãƒƒãƒˆã‚¹ã‚³ã‚¢ã«ã‚ˆã‚‹æœ€é©ã‚¯ãƒ©ã‚¹ã‚¿æ•°æ±ºå®š
- PCAã«ã‚ˆã‚‹æ¬¡å…ƒå‰Šæ¸›
"""

import pandas as pd
import numpy as np
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_score
import config

def load_processed_data():
    """å‰å‡¦ç†æ¸ˆã¿ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã‚€"""
    print("\n" + "="*60)
    print("ğŸ“‚ å‰å‡¦ç†æ¸ˆã¿ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿ä¸­...")
    print("="*60)
    
    try:
        df = pd.read_csv(config.PROCESSED_DATA_FILE)
        print(f"âœ“ ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿å®Œäº†: {len(df)}ä»¶")
        return df
    except FileNotFoundError:
        print("\nâŒ å‰å‡¦ç†æ¸ˆã¿ãƒ‡ãƒ¼ã‚¿ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚")
        print("å…ˆã« data_preprocessing.py ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚")
        return None
    except Exception as e:
        print(f"\nâŒ ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
        return None

def prepare_features(df):
    """ç‰¹å¾´é‡ã‚’æº–å‚™ã™ã‚‹"""
    print("\nğŸ”§ ç‰¹å¾´é‡ã‚’æº–å‚™ä¸­...")
    
    # è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ç‰¹å¾´é‡ã‚«ãƒ©ãƒ ã‚’å–å¾—
    feature_cols = config.FEATURE_COLUMNS
    
    # åˆ©ç”¨å¯èƒ½ãªç‰¹å¾´é‡ã‚«ãƒ©ãƒ ã‚’ç¢ºèª
    available_cols = [col for col in feature_cols if col in df.columns]
    
    if len(available_cols) == 0:
        print("âŒ ç‰¹å¾´é‡ã‚«ãƒ©ãƒ ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚")
        return None, None
    
    print(f"ğŸ“‹ ä½¿ç”¨ã™ã‚‹ç‰¹å¾´é‡ ({len(available_cols)}å€‹):")
    for col in available_cols:
        print(f"   - {col}")
    
    # ç‰¹å¾´é‡ãƒ‡ãƒ¼ã‚¿ã‚’æŠ½å‡º
    X = df[available_cols].copy()
    
    # æ¬ æå€¤ã‚’å¹³å‡å€¤ã§åŸ‹ã‚ã‚‹
    X = X.fillna(X.mean())
    
    # ç„¡é™å¤§å€¤ã‚’é™¤å¤–
    X = X.replace([np.inf, -np.inf], np.nan)
    X = X.fillna(X.mean())
    
    print(f"âœ“ ç‰¹å¾´é‡æº–å‚™å®Œäº†: {X.shape[0]}è¡Œ Ã— {X.shape[1]}åˆ—")
    
    return X, available_cols

def standardize_features(X):
    """ç‰¹å¾´é‡ã‚’æ¨™æº–åŒ–ã™ã‚‹"""
    print("\nğŸ“Š ç‰¹å¾´é‡ã‚’æ¨™æº–åŒ–ä¸­...")
    
    scaler = StandardScaler()
    X_scaled = scaler.fit_transform(X)
    
    print("âœ“ æ¨™æº–åŒ–å®Œäº†")
    
    return X_scaled, scaler

def perform_pca(X_scaled):
    """ä¸»æˆåˆ†åˆ†æã‚’å®Ÿè¡Œã™ã‚‹"""
    print("\nğŸ” PCAã«ã‚ˆã‚‹æ¬¡å…ƒå‰Šæ¸›ä¸­...")
    
    n_components = min(config.PCA_COMPONENTS, X_scaled.shape[1])
    pca = PCA(n_components=n_components)
    X_pca = pca.fit_transform(X_scaled)
    
    explained_variance = pca.explained_variance_ratio_
    cumulative_variance = np.cumsum(explained_variance)
    
    print(f"âœ“ PCAå®Œäº†: {n_components}æ¬¡å…ƒã«å‰Šæ¸›")
    print(f"ğŸ“ˆ èª¬æ˜ã•ã‚ŒãŸåˆ†æ•£:")
    for i, (ev, cv) in enumerate(zip(explained_variance, cumulative_variance)):
        print(f"   PC{i+1}: {ev:.2%} (ç´¯ç©: {cv:.2%})")
    
    return X_pca, pca

def find_optimal_clusters(X_scaled):
    """ã‚·ãƒ«ã‚¨ãƒƒãƒˆã‚¹ã‚³ã‚¢ã§æœ€é©ãªã‚¯ãƒ©ã‚¹ã‚¿æ•°ã‚’æ¢ç´¢ã™ã‚‹"""
    print("\nğŸ¯ æœ€é©ãªã‚¯ãƒ©ã‚¹ã‚¿æ•°ã‚’æ¢ç´¢ä¸­...")
    
    min_k = config.MIN_CLUSTERS
    max_k = min(config.MAX_CLUSTERS, len(X_scaled) - 1)
    
    silhouette_scores = []
    k_values = range(min_k, max_k + 1)
    
    best_k = min_k
    best_score = -1
    
    for k in k_values:
        kmeans = KMeans(n_clusters=k, random_state=config.RANDOM_STATE, n_init=10)
        labels = kmeans.fit_predict(X_scaled)
        score = silhouette_score(X_scaled, labels)
        silhouette_scores.append(score)
        
        print(f"   k={k}: ã‚·ãƒ«ã‚¨ãƒƒãƒˆã‚¹ã‚³ã‚¢ = {score:.4f}")
        
        if score > best_score:
            best_k = k
            best_score = score
    
    print(f"\nâœ… æœ€é©ã‚¯ãƒ©ã‚¹ã‚¿æ•°: k={best_k} (ã‚¹ã‚³ã‚¢: {best_score:.4f})")
    
    return best_k, best_score, silhouette_scores

def perform_clustering(X_scaled, n_clusters):
    """KMeansã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ã‚’å®Ÿè¡Œã™ã‚‹"""
    print(f"\nğŸ¨ KMeansã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ã‚’å®Ÿè¡Œä¸­ (k={n_clusters})...")
    
    kmeans = KMeans(n_clusters=n_clusters, 
                   random_state=config.RANDOM_STATE,
                   n_init=10,
                   max_iter=300)
    
    labels = kmeans.fit_predict(X_scaled)
    
    # ã‚¯ãƒ©ã‚¹ã‚¿ã”ã¨ã®ã‚µãƒ³ãƒ—ãƒ«æ•°ã‚’ç¢ºèª
    unique, counts = np.unique(labels, return_counts=True)
    print("\nğŸ“Š ã‚¯ãƒ©ã‚¹ã‚¿åˆ†å¸ƒ:")
    for cluster_id, count in zip(unique, counts):
        print(f"   ã‚¯ãƒ©ã‚¹ã‚¿ {cluster_id}: {count}ä»¶ ({count/len(labels)*100:.1f}%)")
    
    print("âœ“ ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°å®Œäº†")
    
    return kmeans, labels

def analyze_clusters(df, labels, feature_cols):
    """ã‚¯ãƒ©ã‚¹ã‚¿ã”ã¨ã®ç‰¹å¾´é‡ã‚’åˆ†æã™ã‚‹"""
    print("\nğŸ“ˆ ã‚¯ãƒ©ã‚¹ã‚¿ç‰¹æ€§ã‚’åˆ†æä¸­...")
    
    df_with_cluster = df.copy()
    df_with_cluster['cluster'] = labels
    
    # ã‚¯ãƒ©ã‚¹ã‚¿ã”ã¨ã®ç‰¹å¾´é‡å¹³å‡
    cluster_summary = df_with_cluster.groupby('cluster')[feature_cols].mean()
    
    print("\nğŸ“‹ ã‚¯ãƒ©ã‚¹ã‚¿ã”ã¨ã®ç‰¹å¾´é‡å¹³å‡:")
    print(cluster_summary.to_string())
    
    # ã‚¯ãƒ©ã‚¹ã‚¿è§£é‡ˆ
    print("\nğŸ·ï¸ ã‚¯ãƒ©ã‚¹ã‚¿è§£é‡ˆ:")
    for cluster_id in cluster_summary.index:
        row = cluster_summary.loc[cluster_id]
        
        # ç¶­æŒç®¡ç†å›°é›£åº¦ã‚’åˆ¤å®š
        high_risk_factors = []
        
        if 'bridge_age' in feature_cols and row['bridge_age'] > 50:
            high_risk_factors.append("é«˜æ©‹é½¢")
        
        if 'condition_score' in feature_cols and row['condition_score'] >= 3:
            high_risk_factors.append("å¥å…¨åº¦ä½ä¸‹")
        
        if 'maintenance_priority' in feature_cols and row['maintenance_priority'] > 100:
            high_risk_factors.append("é«˜è£œä¿®å„ªå…ˆåº¦")
        
        if 'population_decline' in feature_cols and row['population_decline'] > 15:
            high_risk_factors.append("äººå£æ¸›å°‘")
        
        if 'aging_rate' in feature_cols and row['aging_rate'] > 35:
            high_risk_factors.append("é«˜é½¢åŒ–")
        
        if 'fiscal_index' in feature_cols and row['fiscal_index'] < 0.5:
            high_risk_factors.append("è²¡æ”¿åŠ›å¼±")
        
        risk_level = "ğŸ”´ é«˜ãƒªã‚¹ã‚¯" if len(high_risk_factors) >= 3 else \
                     "ğŸŸ¡ ä¸­ãƒªã‚¹ã‚¯" if len(high_risk_factors) >= 2 else \
                     "ğŸŸ¢ ä½ãƒªã‚¹ã‚¯"
        
        print(f"\n   ã‚¯ãƒ©ã‚¹ã‚¿ {cluster_id} {risk_level}")
        if high_risk_factors:
            print(f"     ç‰¹å¾´: {', '.join(high_risk_factors)}")
    
    return df_with_cluster, cluster_summary

def save_results(df_with_cluster, cluster_summary):
    """çµæœã‚’ä¿å­˜ã™ã‚‹"""
    print("\nğŸ’¾ çµæœã‚’ä¿å­˜ä¸­...")
    
    # ã‚¯ãƒ©ã‚¹ã‚¿çµæœã‚’ä¿å­˜
    df_with_cluster.to_csv(config.CLUSTER_RESULT_FILE, index=False, encoding='utf-8-sig')
    print(f"âœ“ ã‚¯ãƒ©ã‚¹ã‚¿çµæœ: {config.CLUSTER_RESULT_FILE}")
    
    # ã‚¯ãƒ©ã‚¹ã‚¿ã‚µãƒãƒªãƒ¼ã‚’ä¿å­˜
    cluster_summary.to_csv(config.CLUSTER_SUMMARY_FILE, encoding='utf-8-sig')
    print(f"âœ“ ã‚¯ãƒ©ã‚¹ã‚¿ã‚µãƒãƒªãƒ¼: {config.CLUSTER_SUMMARY_FILE}")

def main():
    """ãƒ¡ã‚¤ãƒ³å‡¦ç†"""
    print("\n" + "="*60)
    print("ğŸš€ æ©‹æ¢ç¶­æŒç®¡ç†ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚° MVP")
    print("="*60)
    
    # 1. ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
    df = load_processed_data()
    if df is None:
        return
    
    # 2. ç‰¹å¾´é‡æº–å‚™
    X, feature_cols = prepare_features(df)
    if X is None:
        return
    
    # 3. æ¨™æº–åŒ–
    X_scaled, scaler = standardize_features(X)
    
    # 4. PCA
    X_pca, pca = perform_pca(X_scaled)
    
    # 5. æœ€é©ã‚¯ãƒ©ã‚¹ã‚¿æ•°æ¢ç´¢
    best_k, best_score, silhouette_scores = find_optimal_clusters(X_scaled)
    
    # 6. ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°å®Ÿè¡Œ
    kmeans, labels = perform_clustering(X_scaled, best_k)
    
    # 7. ã‚¯ãƒ©ã‚¹ã‚¿åˆ†æ
    df_with_cluster, cluster_summary = analyze_clusters(df, labels, feature_cols)
    
    # 8. çµæœä¿å­˜
    save_results(df_with_cluster, cluster_summary)
    
    print("\n" + "="*60)
    print("âœ… å‡¦ç†å®Œäº†ï¼")
    print("="*60)
    print(f"\nğŸ“ å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«:")
    print(f"   - {config.CLUSTER_RESULT_FILE}")
    print(f"   - {config.CLUSTER_SUMMARY_FILE}")
    print(f"\nğŸ’¡ æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—: visualization.py ã‚’å®Ÿè¡Œã—ã¦çµæœã‚’å¯è¦–åŒ–ã—ã¦ãã ã•ã„ã€‚")
    
    return df_with_cluster, cluster_summary, X_pca, labels

if __name__ == "__main__":
    main()
